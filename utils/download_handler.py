#!/usr/bin/env python
"""Utility module for handling downloads with retry logic and progress monitoring."""

import asyncio
import logging
import time
import tempfile
import numpy as np
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime, timedelta

import httpx
import pandas as pd
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)

from utils.logger_setup import get_logger
from utils.time_alignment import TimeRangeManager

logger = get_logger(__name__, "INFO", show_path=False, rich_tracebacks=True)


class DownloadProgressTracker:
    """Tracks download progress and detects stalled downloads."""

    def __init__(self, total_size: Optional[int] = None, check_interval: int = 5):
        """Initialize progress tracker.

        Args:
            total_size: Expected total size in bytes, if known
            check_interval: How often to check progress in seconds
        """
        self.start_time = time.monotonic()
        self.last_progress_time = self.start_time
        self.bytes_received = 0
        self.total_size = total_size
        self.last_bytes = 0
        self.check_interval = check_interval

    def update(self, chunk_size: int) -> bool:
        """Update progress and check if download is stalled.

        Args:
            chunk_size: Size of the latest chunk in bytes

        Returns:
            False if download appears stalled, True otherwise
        """
        current_time = time.monotonic()
        self.bytes_received += chunk_size

        # Check progress every check_interval seconds
        if current_time - self.last_progress_time >= self.check_interval:
            bytes_per_sec = (
                self.bytes_received - self.last_bytes
            ) / self.check_interval

            # If progress is less than 1KB/s for check_interval seconds, consider it stalled
            if bytes_per_sec < 1024:
                logger.warning(
                    f"Download stalled: {bytes_per_sec:.2f} B/s "
                    f"({self.bytes_received}/{self.total_size or 'unknown'} bytes)"
                )
                return False

            self.last_progress_time = current_time
            self.last_bytes = self.bytes_received

        return True


class DownloadStalledException(Exception):
    """Raised when download progress stalls."""

    pass


class RateLimitException(Exception):
    """Raised when rate limited by the server."""

    pass


class DownloadHandler:
    """Handles file downloads with retry logic and progress monitoring."""

    def __init__(
        self,
        client: httpx.AsyncClient,
        max_retries: int = 5,
        min_wait: int = 4,
        max_wait: int = 60,
        chunk_size: int = 8192,
    ):
        """Initialize download handler.

        Args:
            client: Async HTTP client to use
            max_retries: Maximum number of retry attempts
            min_wait: Minimum wait time between retries (seconds)
            max_wait: Maximum wait time between retries (seconds)
            chunk_size: Size of download chunks in bytes
        """
        self.client = client
        self.max_retries = max_retries
        self.min_wait = min_wait
        self.max_wait = max_wait
        self.chunk_size = chunk_size

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        retry=retry_if_exception_type(
            (
                DownloadStalledException,
                RateLimitException,
                httpx.TimeoutException,
                httpx.NetworkError,
            )
        ),
        before_sleep=before_sleep_log(logger, logging.WARNING),
    )
    async def download_file(
        self,
        url: str,
        local_path: Path,
        headers: Optional[Dict[str, Any]] = None,
        progress_tracker_kwargs: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """Download file with progress monitoring and retries.

        Args:
            url: URL to download from
            local_path: Path to save the file to
            headers: Optional HTTP headers
            progress_tracker_kwargs: Optional kwargs for DownloadProgressTracker

        Returns:
            True if download successful, False otherwise

        Raises:
            DownloadStalledException: If download progress stalls
            RateLimitException: If rate limited by server
            httpx.TimeoutException: If request times out
            httpx.NetworkError: If network error occurs
        """
        temp_path = local_path.with_suffix(".tmp")
        progress_tracker_kwargs = progress_tracker_kwargs or {}

        try:
            async with self.client.stream("GET", url, headers=headers) as response:
                if response.status_code == 429:
                    retry_after = int(response.headers.get("Retry-After", 60))
                    logger.warning(f"Rate limit hit. Retry after {retry_after}s")
                    await asyncio.sleep(retry_after)  # Honor the retry-after header
                    raise RateLimitException()

                if response.status_code != 200:
                    logger.error(f"Download failed with status {response.status_code}")
                    return False

                total_size = int(response.headers.get("content-length", 0))
                progress = DownloadProgressTracker(
                    total_size, **progress_tracker_kwargs
                )

                with open(temp_path, "wb") as f:
                    async for chunk in response.aiter_bytes(self.chunk_size):
                        if not progress.update(len(chunk)):
                            msg = f"Download stalled at {progress.bytes_received}/{total_size} bytes"
                            logger.warning(msg)
                            raise DownloadStalledException(msg)
                        f.write(chunk)

            # Only move to final location if download completed successfully
            temp_path.rename(local_path)
            return True

        except (
            DownloadStalledException,
            RateLimitException,
            httpx.TimeoutException,
            httpx.NetworkError,
        ) as e:
            # Let these exceptions propagate for retry
            raise

        except Exception as e:
            logger.error(f"Unexpected error during download: {str(e)}")
            return False

        finally:
            # Cleanup temp file if it exists
            temp_path.unlink(missing_ok=True)


class VisionDownloadManager:
    """Handles downloading Vision data files with validation and processing."""

    def __init__(self, client: httpx.AsyncClient, symbol: str, interval: str):
        """Initialize the download manager.

        Args:
            client: HTTP client for downloads
            symbol: Trading pair symbol
            interval: Time interval
        """
        self.client = client
        self.symbol = symbol
        self.interval = interval
        self.download_handler = DownloadHandler(
            client, max_retries=5, min_wait=4, max_wait=60
        )

    def _get_checksum_url(self, date: datetime) -> str:
        """Get checksum URL for a specific date.

        Args:
            date: Target date

        Returns:
            URL for the checksum file
        """
        # Import and use TimeRangeManager for consistent timezone handling
        date = TimeRangeManager.enforce_utc_timezone(date)

        # Note: This assumes the vision_constraints module has this function
        from core.vision_constraints import get_vision_url, FileType

        return get_vision_url(self.symbol, self.interval, date, FileType.CHECKSUM)

    def _get_data_url(self, date: datetime) -> str:
        """Get data URL for a specific date.

        Args:
            date: Target date

        Returns:
            URL for the data file
        """
        # Import and use TimeRangeManager for consistent timezone handling
        date = TimeRangeManager.enforce_utc_timezone(date)

        # Note: This assumes the vision_constraints module has this function
        from core.vision_constraints import get_vision_url, FileType

        return get_vision_url(self.symbol, self.interval, date, FileType.DATA)

    def _verify_checksum(self, file_path: Path, checksum_path: Path) -> bool:
        """Verify file checksum.

        Args:
            file_path: Path to data file
            checksum_path: Path to checksum file

        Returns:
            Verification status
        """
        try:
            with open(checksum_path, "r") as f:
                expected = f.read().strip().split()[0]

            from utils.cache_validator import CacheValidator

            return CacheValidator.validate_cache_checksum(file_path, expected)
        except Exception as e:
            logger.error(f"Error verifying checksum: {e}")
            return False

    async def download_file(self, url: str, local_path: Path) -> bool:
        """Download a file from URL to local path.

        Args:
            url: URL to download from
            local_path: Path to save to

        Returns:
            True if download successful, False otherwise
        """
        try:
            return await self.download_handler.download_file(url, local_path)
        except Exception as e:
            logger.error(f"Error downloading {url}: {str(e)}")
            return False

    async def download_date(self, date: datetime) -> Optional[pd.DataFrame]:
        """Download data for a specific date.

        Args:
            date: Target date

        Returns:
            DataFrame with data or None if download failed
        """
        # Ensure date has proper timezone using TimeRangeManager
        date = TimeRangeManager.enforce_utc_timezone(date)

        # Create temporary directory for downloads
        temp_dir = Path(tempfile.mkdtemp())
        data_file = (
            temp_dir
            / f"{self.symbol}_{self.interval}_{date.strftime('%Y%m%d')}_data.zip"
        )
        checksum_file = (
            temp_dir
            / f"{self.symbol}_{self.interval}_{date.strftime('%Y%m%d')}_checksum"
        )

        try:
            # Download data and checksum files
            data_url = self._get_data_url(date)
            checksum_url = self._get_checksum_url(date)

            logger.info(f"Downloading data for {date.strftime('%Y-%m-%d')} from:")
            logger.info(f"Data: {data_url}")
            logger.info(f"Checksum: {checksum_url}")

            success = await asyncio.gather(
                self.download_file(data_url, data_file),
                self.download_file(checksum_url, checksum_file),
            )

            if not all(success):
                logger.error(f"Failed to download files for {date}")
                return None

            # Verify checksum
            if not self._verify_checksum(data_file, checksum_file):
                logger.error(f"Checksum verification failed for {date}")
                return None

            # Read CSV data with detailed error handling
            try:
                logger.info(f"Reading CSV data from {data_file}")
                df = pd.read_csv(
                    data_file,
                    compression="zip",
                    names=[
                        "open_time",
                        "open",
                        "high",
                        "low",
                        "close",
                        "volume",
                        "close_time",
                        "quote_volume",
                        "trades",
                        "taker_buy_volume",
                        "taker_buy_quote_volume",
                        "ignored",
                    ],
                )

                if df.empty:
                    logger.error(f"Empty DataFrame after reading CSV for {date}")
                    return None

                # Detect timestamp format and convert
                from core.vision_constraints import detect_timestamp_unit

                sample_ts = df["open_time"].iloc[0]
                ts_unit = detect_timestamp_unit(sample_ts)

                # Convert timestamps
                df["open_time"] = pd.to_datetime(df["open_time"], unit=ts_unit)
                df["open_time"] = df["open_time"].dt.floor("s") + pd.Timedelta(
                    microseconds=0
                )

                # Handle close_time
                df["close_time"] = df["close_time"].astype(np.int64)
                if len(str(df["close_time"].iloc[0])) == 19:  # nanoseconds
                    df["close_time"] = (
                        df["close_time"] // 1000
                    )  # Convert to microseconds
                df["close_time"] = (df["close_time"].astype(np.int64) + 999) * 1000

                # Set index
                df.set_index("open_time", inplace=True)
                df = df.drop(columns=["ignored"])

                # Ensure UTC timezone using TimeRangeManager
                if df.index.tz is None:
                    df.index = pd.DatetimeIndex(
                        [TimeRangeManager.enforce_utc_timezone(dt) for dt in df.index],
                        name=df.index.name,
                    )
                else:
                    df.index = pd.DatetimeIndex(
                        [
                            TimeRangeManager.enforce_utc_timezone(dt)
                            for dt in df.index.to_pydatetime()
                        ],
                        name=df.index.name,
                    )

                return df

            except pd.errors.EmptyDataError:
                logger.error(f"Empty data file for {date}")
                return None
            except Exception as e:
                logger.error(f"Error processing data for {date}: {str(e)}")
                logger.error("Error details:", exc_info=True)
                return None

        except Exception as e:
            logger.error(f"Error downloading data for {date}: {str(e)}")
            logger.error("Error details:", exc_info=True)
            return None

        finally:
            # Cleanup temporary files
            try:
                data_file.unlink(missing_ok=True)
                checksum_file.unlink(missing_ok=True)
                temp_dir.rmdir()
            except Exception as e:
                logger.error(f"Error cleaning up temporary files: {str(e)}")
