#!/usr/bin/env python
r"""VisionDataClient provides direct access to Binance Vision API for historical data.

This module implements a client for retrieving historical market data from the
Binance Vision API. It provides functions for fetching, validating, and processing data.

Functionality:
- Fetch historical market data by symbol, interval, and time range
- Validate data integrity and structure
- Process data into pandas DataFrames for analysis

The VisionDataClient is primarily used through the DataSourceManager, which provides
a unified interface for data retrieval with automatic source selection and caching.

For most use cases, users should interact with the DataSourceManager rather than
directly with this client.
"""

from datetime import datetime, timedelta, timezone
from typing import Optional, Sequence, TypeVar, Generic, Union, List, Dict, Tuple
import os
import tempfile
import zipfile
import httpx
from concurrent.futures import ThreadPoolExecutor, as_completed
import re

import pandas as pd

from utils.logger_setup import logger
from utils.market_constraints import Interval, MarketType, ChartType, DataProvider
from utils.time_utils import (
    filter_dataframe_by_time,
    detect_timestamp_unit,
)
from utils.config import (
    KLINE_COLUMNS,
    MAXIMUM_CONCURRENT_DOWNLOADS,
    FileType,
)
from utils.dataframe_types import TimestampedDataFrame
from core.sync.vision_constraints import get_vision_url
from utils.gap_detector import detect_gaps, Gap
from utils.dataframe_utils import ensure_open_time_as_column
from core.sync.data_client_interface import DataClientInterface
from utils.validation import DataFrameValidator

# Define the type variable for VisionDataClient
T = TypeVar("T")


class VisionDataClient(DataClientInterface, Generic[T]):
    """Vision Data Client for direct access to Binance historical data.

    Important note on timestamp semantics:
    - open_time represents the BEGINNING of the candle period (standard in financial data)
    - close_time represents the END of the candle period
    - This implementation preserves this semantic meaning across all interval types
    """

    def __init__(
        self,
        symbol: str,
        interval: str = "1s",
        market_type: Union[str, MarketType] = MarketType.SPOT,
    ):
        """Initialize Vision Data Client.

        Args:
            symbol: Trading symbol e.g. 'BTCUSDT'
            interval: Kline interval e.g. '1s', '1m'
            market_type: Market type (SPOT, FUTURES_USDT, FUTURES_COIN) or string
        """
        self._symbol = symbol.upper()
        self._interval_str = interval
        self.market_type = market_type

        # Convert MarketType enum to string if needed
        market_type_str = market_type
        if isinstance(market_type, MarketType):
            try:
                market_name = market_type.name
                if market_name == "SPOT":
                    market_type_str = "spot"
                elif market_name == "FUTURES_USDT":
                    market_type_str = "futures_usdt"
                elif market_name == "FUTURES_COIN":
                    market_type_str = "futures_coin"
                elif market_name == "FUTURES":
                    market_type_str = "futures_usdt"  # Default to USDT for legacy type
                else:
                    raise ValueError(f"Unsupported market type: {market_type}")
            except (AttributeError, TypeError):
                # Fallback to string representation for safer comparison
                market_str = str(market_type).upper()
                if "SPOT" in market_str:
                    market_type_str = "spot"
                elif "FUTURES_USDT" in market_str or "FUTURES" == market_str:
                    market_type_str = "futures_usdt"
                elif "FUTURES_COIN" in market_str:
                    market_type_str = "futures_coin"
                else:
                    raise ValueError(f"Unsupported market type: {market_type}")

        self.market_type_str = market_type_str

        # Parse interval string to Interval object
        self.interval_obj = self._parse_interval(interval)

        # Create httpx client instead of requests Session
        self._client = httpx.Client(
            timeout=30.0,  # Increased timeout for better reliability
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, application/zip",
            },
            follow_redirects=True,  # Automatically follow redirects
        )

    def __enter__(self) -> "VisionDataClient":
        """Context manager entry."""
        return self

    def __exit__(self, _exc_type, _exc_val, _exc_tb):
        """Context manager exit."""
        # Release resources
        if hasattr(self, "_client") and self._client:
            if hasattr(self._client, "close") and callable(self._client.close):
                self._client.close()

    def close(self) -> None:
        """Close the client and release resources."""
        if hasattr(self, "_client") and self._client:
            if hasattr(self._client, "close") and callable(self._client.close):
                self._client.close()
                self._client = None
                logger.debug("Closed Vision API HTTP client")

    @property
    def provider(self) -> DataProvider:
        """Get the data provider for this client."""
        return DataProvider.BINANCE

    @property
    def chart_type(self) -> ChartType:
        """Get the chart type for this client."""
        return ChartType.KLINES

    @property
    def symbol(self) -> str:
        """Get the symbol for this client."""
        return self._symbol

    @property
    def interval(self) -> Union[str, object]:
        """Get the interval for this client."""
        return self._interval_str

    def create_empty_dataframe(self) -> pd.DataFrame:
        """Create an empty dataframe with the correct structure.

        Returns:
            Empty TimestampedDataFrame with the correct columns
        """
        # Use the standardized empty dataframe function from config
        from utils.config import create_empty_dataframe

        # Create empty dataframe and convert to TimestampedDataFrame format
        df = create_empty_dataframe()

        # Set index to open_time_us (required by TimestampedDataFrame)
        if "open_time_us" not in df.columns:
            df["open_time_us"] = pd.Series(dtype="int64")
            df = df.set_index("open_time_us")

        return TimestampedDataFrame(df)

    def is_data_available(self, start_time: datetime, end_time: datetime) -> bool:
        """Check if data is available for the specified time range.

        Args:
            start_time: Start time
            end_time: End time

        Returns:
            True if data is available, False otherwise
        """
        # For Binance Vision API, data is available from the start of the exchange
        # (September 2017 for most pairs), up to around 24-48 hours ago
        launch_date = datetime(2017, 9, 1, tzinfo=timezone.utc)

        # Check if the requested time range is after the launch date
        if end_time < launch_date:
            return False

        # Check if the requested time range is too recent
        # Vision data typically has a 24-48 hour delay
        now = datetime.now(timezone.utc)
        vision_cutoff = now - timedelta(hours=48)
        if start_time > vision_cutoff:
            return False

        # Otherwise, data should be available
        return True

    def validate_data(self, df: pd.DataFrame) -> Tuple[bool, Optional[str]]:
        """Validate that a DataFrame contains valid market data.

        Args:
            df: DataFrame to validate

        Returns:
            Tuple of (is_valid, error_message)
        """
        validator = DataFrameValidator(df)
        return validator.validate_klines_data()

    def _parse_interval(self, interval_str: str) -> Interval:
        """Parse and validate interval string against market_constraints.Interval.

        Args:
            interval_str: Interval string (e.g., "1m", "1h")

        Returns:
            Parsed Interval enum

        Raises:
            ValueError: If interval is invalid or not supported
        """
        try:
            # Try to find the interval enum by value
            interval_obj = next((i for i in Interval if i.value == interval_str), None)
            if interval_obj is None:
                # Try by enum name (upper case with _ instead of number)
                try:
                    interval_obj = Interval[interval_str.upper()]
                except KeyError:
                    raise ValueError(f"Invalid interval: {interval_str}")

            # Check if this interval is supported for this market type
            # Could implement is_interval_supported() function if needed
            logger.debug(
                f"Using interval {interval_obj.name} ({interval_obj.value}) for {self.market_type_str}"
            )

            return interval_obj
        except Exception as e:
            logger.error(f"Error parsing interval {interval_str}: {e}")
            # Default to 1s as a failsafe
            return Interval.SECOND_1

    def _get_interval_seconds(self, interval: str) -> int:
        """Get interval duration in seconds from interval string.

        This method handles converting string intervals directly to seconds
        without requiring the MarketInterval enum object.

        Args:
            interval: Interval string (e.g., "1s", "1m", "1h")

        Returns:
            Number of seconds in the interval
        """
        # Parse interval value and unit
        match = re.match(r"(\d+)([smhdwM])", interval)
        if not match:
            raise ValueError(f"Invalid interval format: {interval}")

        num, unit = match.groups()
        num = int(num)

        # Define multipliers for each unit
        multipliers = {
            "s": 1,
            "m": 60,
            "h": 3600,
            "d": 86400,
            "w": 604800,
            "M": 2592000,  # Approximate - using 30 days
        }

        if unit not in multipliers:
            raise ValueError(f"Unknown interval unit: {unit}")

        return num * multipliers[unit]

    def _validate_timestamp_safety(self, date: datetime) -> bool:
        """Check if a given timestamp is safe to use with pandas datetime conversion.

        Args:
            date: The datetime to check

        Returns:
            True if the timestamp is safe, False if it might cause out-of-bounds errors

        Note:
            Pandas can have issues with timestamps very far in the future due to
            nanosecond conversion limitations. This check helps prevent those issues.
        """
        try:
            # Check if date is within pandas timestamp limits
            # The max timestamp supported is approximately year 2262
            max_safe_year = 2262
            if date.year > max_safe_year:
                logger.warning(
                    f"Date {date.isoformat()} exceeds pandas timestamp safe year limit ({max_safe_year})"
                )
                return False

            # Test conversion to pandas timestamp to see if it would raise an error
            _ = pd.Timestamp(date)
            return True
        except (OverflowError, ValueError, pd.errors.OutOfBoundsDatetime) as e:
            logger.warning(
                f"Date {date.isoformat()} caused timestamp validation error: {e}"
            )
            return False

    def _process_timestamp_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process timestamp columns in the dataframe, handling various formats.

        This method preserves the exact timestamps from the raw data without any shifting:
        - open_time represents the BEGINNING of a candle period
        - close_time represents the END of the candle period

        Args:
            df: DataFrame with timestamp columns to process

        Returns:
            DataFrame with processed timestamp columns
        """
        if df.empty:
            return df

        try:
            # Check timestamp format if dataframe has rows
            if len(df) > 0:
                # Debug: Log first few raw rows to track data through the pipeline
                logger.debug(
                    f"[TIMESTAMP TRACE] Input data to _process_timestamp_columns has {len(df)} rows"
                )
                for i in range(min(3, len(df))):
                    logger.debug(
                        f"[TIMESTAMP TRACE] Raw row {i}: open_time={df.iloc[i, 0]}, close={df.iloc[i, 4]}"
                    )

                first_ts = df.iloc[0, 0]  # First timestamp in first column
                last_ts = df.iloc[-1, 0] if len(df) > 1 else first_ts

                logger.debug(f"First raw timestamp detected: {first_ts}")
                logger.debug(f"Last raw timestamp detected: {last_ts}")

                try:
                    # Detect timestamp unit using the standardized function from utils.time_utils
                    timestamp_unit = detect_timestamp_unit(first_ts)

                    # Log timestamp details for debugging
                    logger.debug(f"First timestamp: {first_ts} ({timestamp_unit})")
                    if len(df) > 1:
                        last_ts = df.iloc[-1, 0]
                        logger.debug(f"Last timestamp: {last_ts} ({timestamp_unit})")

                    # Convert timestamps to datetime, preserving their semantic meaning:
                    # - open_time (1st column) is the BEGINNING of the candle period
                    # - close_time (7th column) is the END of the candle period
                    if "open_time" in df.columns:
                        df["open_time"] = pd.to_datetime(
                            df["open_time"], unit=timestamp_unit, utc=True
                        )
                        logger.debug(
                            f"Converted open_time: first value = {df['open_time'].iloc[0]} (BEGINNING of candle)"
                        )
                        # Debug: Log first few converted timestamps to track processing
                        for i in range(min(3, len(df))):
                            logger.debug(
                                f"[TIMESTAMP TRACE] Converted row {i}: open_time={df['open_time'].iloc[i]}, close={df.iloc[i, 4]}"
                            )

                    if "close_time" in df.columns:
                        df["close_time"] = pd.to_datetime(
                            df["close_time"], unit=timestamp_unit, utc=True
                        )
                        logger.debug(
                            f"Converted close_time: first value = {df['close_time'].iloc[0]} (END of candle)"
                        )

                    # Verify timestamp semantics are preserved (for debugging)
                    if (
                        "open_time" in df.columns
                        and "close_time" in df.columns
                        and len(df) > 0
                    ):
                        first_open = df["open_time"].iloc[0]
                        first_close = df["close_time"].iloc[0]
                        time_diff = (first_close - first_open).total_seconds()

                        # Calculate expected difference based on interval
                        # For 1s interval, close should be 0.999 seconds after open
                        # For 1m interval, close should be 59.999 seconds after open, etc.
                        expected_diff = (
                            self._get_interval_seconds(self._interval_str) - 0.001
                        )

                        logger.debug(
                            f"Time difference between first open_time and close_time: {time_diff:.3f}s "
                            f"(expected ~{expected_diff:.3f}s for {self._interval_str} interval)"
                        )

                        # Verify the time difference is within expected range
                        # Allow for a small tolerance to account for precision differences
                        tolerance = 0.1  # 100ms tolerance
                        if abs(time_diff - expected_diff) > tolerance:
                            logger.warning(
                                f"Unexpected time difference between open_time and close_time: "
                                f"{time_diff:.3f}s vs expected {expected_diff:.3f}s for {self._interval_str} interval. "
                                f"This could indicate a timestamp interpretation issue."
                            )
                        else:
                            logger.debug(
                                f"Time difference between open_time and close_time is as expected "
                                f"({time_diff:.3f}s) for {self._interval_str} interval."
                            )

                        logger.debug(
                            f"Timestamps converted preserving their semantic meaning: "
                            f"open_time=BEGINNING of candle, close_time=END of candle"
                        )

                except ValueError as e:
                    logger.warning(f"Error detecting timestamp unit: {e}")
                    # Fall back to default handling with microseconds as unit
                    logger.warning("Falling back to microseconds as the timestamp unit")
                    if "open_time" in df.columns:
                        df["open_time"] = pd.to_datetime(
                            df["open_time"], unit="us", utc=True
                        )
                        logger.debug(
                            f"Converted open_time using fallback method: first value = {df['open_time'].iloc[0]} (BEGINNING of candle)"
                        )
                    if "close_time" in df.columns:
                        df["close_time"] = pd.to_datetime(
                            df["close_time"], unit="us", utc=True
                        )
                        logger.debug(
                            f"Converted close_time using fallback method: first value = {df['close_time'].iloc[0]} (END of candle)"
                        )

            # Debug: Log output from timestamp processing
            logger.debug(
                f"[TIMESTAMP TRACE] After _process_timestamp_columns: {len(df)} rows"
            )
            if len(df) > 0 and "open_time" in df.columns:
                for i in range(min(3, len(df))):
                    logger.debug(
                        f"[TIMESTAMP TRACE] Processed row {i}: open_time={df['open_time'].iloc[i]}, close={df.iloc[i, 4]}"
                    )

        except Exception as e:
            logger.error(f"Error processing timestamp columns: {e}")

        return df

    def _download_file(
        self, date: datetime
    ) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """Download a data file for a specific date.

        Args:
            date: Date to download data for

        Returns:
            Tuple of (DataFrame, warning message). DataFrame is None if download failed.
        """
        logger.debug(
            f"Downloading data for {date.date()} for {self._symbol} {self._interval_str}"
        )

        temp_file_path = None

        try:
            # Create the file URL
            base_interval = (
                "1m" if self._interval_str == "1s" else self._interval_str
            )  # 1s data stored with filename as 1m
            url = get_vision_url(
                symbol=self._symbol,
                interval=base_interval,
                date=date,
                file_type=FileType.DATA,
                market_type=self.market_type_str,
            )

            # Create a temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=".zip") as temp_file:
                temp_file_path = temp_file.name

            # Download the file
            response = self._client.get(url)
            if response.status_code == 404:
                return None, f"404: Data not available for {date.date()}"

            if response.status_code != 200:
                return None, f"HTTP error {response.status_code} for {date.date()}"

            # Save to the temporary file
            with open(temp_file_path, "wb") as f:
                f.write(response.content)

            # Process the zip file
            try:
                with zipfile.ZipFile(temp_file_path, "r") as zip_ref:
                    # Find the CSV file in the zip
                    csv_files = [f for f in zip_ref.namelist() if f.endswith(".csv")]
                    if not csv_files:
                        return None, f"No CSV file found in zip for {date.date()}"

                    csv_file = csv_files[0]  # Take the first CSV file

                    # Extract and process the CSV file
                    with tempfile.TemporaryDirectory() as temp_dir:
                        zip_ref.extract(csv_file, temp_dir)
                        csv_path = os.path.join(temp_dir, csv_file)

                        # Read the CSV file
                        # DEBUG: Log the first few lines of the raw CSV to check content
                        with open(csv_path, "r") as f:
                            first_lines = [next(f) for _ in range(3)]
                            logger.debug(f"[CSV TRACE] First few lines of raw CSV:")
                            for i, line in enumerate(first_lines):
                                logger.debug(f"[CSV TRACE] Line {i}: {line.strip()}")

                            # Check if the first line contains headers (e.g., 'high' keyword)
                            has_header = any(
                                "high" in line.lower() for line in first_lines[:1]
                            )
                            logger.debug(f"[CSV TRACE] Headers detected: {has_header}")

                            # Reopen the file to read from the beginning
                            f.seek(0)

                        # Read CSV with or without header based on detection
                        if has_header:
                            logger.info(
                                f"Headers detected in CSV, reading with header=0"
                            )
                            df = pd.read_csv(csv_path, header=0)
                            # Map column names to standard names if needed
                            if "open_time" not in df.columns and len(df.columns) == len(
                                KLINE_COLUMNS
                            ):
                                df.columns = KLINE_COLUMNS
                        else:
                            # No headers detected, use the standard column names
                            logger.info(
                                f"No headers detected in CSV, reading with header=None"
                            )
                            df = pd.read_csv(csv_path, header=None, names=KLINE_COLUMNS)

                        # DEBUG: Check if first row contains 00:00:00 timestamp
                        if not df.empty:
                            first_ts = df.iloc[0, 0]  # First column is open_time
                            first_ts_dt = datetime.fromtimestamp(
                                (
                                    first_ts / 1000000
                                    if len(str(int(first_ts))) >= 16
                                    else first_ts / 1000
                                ),
                                tz=timezone.utc,
                            )
                            logger.debug(
                                f"[CSV TRACE] First timestamp in loaded CSV: {first_ts} ({first_ts_dt})"
                            )

                        logger.debug(f"Read {len(df)} rows from CSV")

                        # Process the data
                        if not df.empty:
                            # Store original timestamp info for later analysis if not already present
                            if "original_timestamp" not in df.columns:
                                df["original_timestamp"] = df.iloc[:, 0].astype(str)

                            # Process timestamp columns
                            df = self._process_timestamp_columns(df)

                            return df, None
                        else:
                            return None, f"Empty dataframe for {date.date()}"
            except Exception as e:
                logger.error(
                    f"Error processing zip file {temp_file_path}: {str(e)}",
                    exc_info=True,
                )
                return None, f"Error processing zip file: {str(e)}"
        except Exception as e:
            logger.error(f"Unexpected error processing {date.date()}: {str(e)}")
            return None, f"Unexpected error: {str(e)}"
        finally:
            # Clean up temp file
            if temp_file_path and os.path.exists(temp_file_path):
                os.unlink(temp_file_path)

    def _download_data(
        self,
        start_time: datetime,
        end_time: datetime,
        columns: Optional[Sequence[str]] = None,
    ) -> TimestampedDataFrame:
        """Download data from Binance Vision API for a specific time range.

        Args:
            start_time: Start time
            end_time: End time
            columns: Optional column names to use

        Returns:
            TimestampedDataFrame with downloaded data

        Note:
            Timestamps in the returned data preserve their semantic meaning:
            - open_time represents the BEGINNING of each candle period
            - close_time represents the END of each candle period
        """
        logger.info(
            f"Downloading data for {self._symbol} {self._interval_str} from {start_time.isoformat()} to {end_time.isoformat()}"
        )

        # Debug: Log exact timestamps for requested range
        logger.debug(
            f"[TIMESTAMP TRACE] Requested time range: start={start_time.isoformat()} end={end_time.isoformat()}"
        )

        # Convert start and end times to date objects for file-based lookups
        start_date = start_time.date()
        end_date = end_time.date()

        date_range = []
        current_date = start_date
        while current_date <= end_date:
            date_range.append(current_date)
            current_date = current_date + timedelta(days=1)

        logger.info(f"Need to check {len(date_range)} dates for data")

        # Use ThreadPoolExecutor to download files in parallel
        max_workers = min(MAXIMUM_CONCURRENT_DOWNLOADS, len(date_range))
        downloaded_dfs = []
        warning_messages = []  # Collect warning messages

        # For very short intervals like 1s, avoid too many concurrent downloads
        if self._interval_str == "1s" and max_workers > 10:
            max_workers = 10
            logger.info(
                f"Limited concurrent downloads to {max_workers} for 1s interval"
            )

        # Create date objects to pass to ThreadPoolExecutor
        date_objects = [
            datetime(d.year, d.month, d.day, tzinfo=timezone.utc) for d in date_range
        ]

        # Get data files
        if len(date_objects) == 0:
            logger.warning("No dates to download")
            return self.create_empty_dataframe()

        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit download tasks
                future_to_date = {
                    executor.submit(self._download_file, date_obj): date_obj
                    for date_obj in date_objects
                }

                # Process results as they complete
                for future in as_completed(future_to_date):
                    date = future_to_date[future]
                    try:
                        df, warning = future.result()
                        if warning:
                            warning_messages.append(warning)
                        if df is not None and not df.empty:
                            # Ensure each dataframe is properly sorted by open_time before adding it
                            if (
                                "open_time" in df.columns
                                and not df["open_time"].is_monotonic_increasing
                            ):
                                df = df.sort_values("open_time").reset_index(drop=True)
                            downloaded_dfs.append(df)
                    except Exception as exc:
                        logger.error(f"Error downloading data for {date}: {exc}")
        except Exception as e:
            logger.error(f"Error in ThreadPoolExecutor: {e}")

        # If no data was downloaded, log a consolidated warning and return empty dataframe
        if not downloaded_dfs:
            # Check if we have specific warnings about missing dates
            if warning_messages:
                # Group warnings about 404 (missing data)
                missing_dates = [
                    msg.split(": ")[1].split(" ")[0]
                    for msg in warning_messages
                    if "404" in msg
                ]
                if missing_dates:
                    # Log a consolidated warning instead of individual ones
                    dates_str = ", ".join(missing_dates)
                    logger.warning(
                        f"No data downloaded from Binance Vision API - dates not available: {dates_str}. "
                        f"This may happen for recent data or less common markets. "
                        f"The system will attempt to fetch from REST API instead."
                    )
                else:
                    # Log a generic warning for other issues
                    logger.warning(
                        "No data downloaded from Binance Vision API - this may happen for recent data or less common markets. "
                        "The system will attempt to fetch from REST API instead."
                    )
            else:
                # No specific warnings
                logger.warning(
                    "No data downloaded from Binance Vision API - this may happen for recent data or less common markets. "
                    "The system will attempt to fetch from REST API instead."
                )
            return self.create_empty_dataframe()

        logger.info(f"Downloaded {len(downloaded_dfs)} daily files")

        # Concatenate all dataframes
        concatenated_df = pd.concat(downloaded_dfs, ignore_index=True)

        # Debug: Log the first few rows after concatenation
        if not concatenated_df.empty:
            logger.debug(
                f"[TIMESTAMP TRACE] After concatenation: {len(concatenated_df)} rows total"
            )
            for i in range(min(3, len(concatenated_df))):
                if "open_time" in concatenated_df.columns:
                    logger.debug(
                        f"[TIMESTAMP TRACE] Concatenated row {i}: open_time={concatenated_df['open_time'].iloc[i]}, close={concatenated_df.iloc[i, concatenated_df.columns.get_loc('close')]}"
                    )
        else:
            logger.debug("[TIMESTAMP TRACE] Concatenated DataFrame is empty")

        # If the dataframe is empty, return early
        if concatenated_df.empty:
            logger.warning("No data in downloaded files")
            return self.create_empty_dataframe()

        # Ensure timestamps are in datetime format
        if "open_time" not in concatenated_df.columns:
            logger.error(
                f"Missing 'open_time' column in downloaded data. Columns: {concatenated_df.columns}"
            )
            return self.create_empty_dataframe()

        # Sort the dataframe by timestamp
        if not concatenated_df["open_time"].is_monotonic_increasing:
            concatenated_df = concatenated_df.sort_values("open_time").reset_index(
                drop=True
            )

        # Debug: Log data before filtering
        logger.debug(
            f"[TIMESTAMP TRACE] Before time filtering: {len(concatenated_df)} rows"
        )
        if not concatenated_df.empty:
            min_time = concatenated_df["open_time"].min()
            max_time = concatenated_df["open_time"].max()
            logger.debug(
                f"[TIMESTAMP TRACE] Time range in data before filtering: {min_time} to {max_time}"
            )
            logger.debug(
                f"[TIMESTAMP TRACE] Time range requested: {start_time} to {end_time}"
            )

            # Log first few rows before filtering
            for i in range(min(3, len(concatenated_df))):
                logger.debug(
                    f"[TIMESTAMP TRACE] Before filtering row {i}: open_time={concatenated_df['open_time'].iloc[i]}, close={concatenated_df.iloc[i, concatenated_df.columns.get_loc('close')]}"
                )

            # Check specifically for the exact start time
            exact_match_rows = concatenated_df[
                concatenated_df["open_time"] == start_time
            ]
            if not exact_match_rows.empty:
                logger.debug(
                    f"[TIMESTAMP TRACE] Found {len(exact_match_rows)} rows exactly matching start_time={start_time}"
                )
            else:
                closest_time_idx = (
                    (concatenated_df["open_time"] - start_time).abs().idxmin()
                )
                closest_time = concatenated_df.iloc[closest_time_idx]["open_time"]
                logger.debug(
                    f"[TIMESTAMP TRACE] No exact match for start_time. Closest time is {closest_time}"
                )

                # Check for rows at the expected interval boundaries
                boundary_rows = concatenated_df[
                    (concatenated_df["open_time"].dt.minute == 0)
                    & (concatenated_df["open_time"].dt.second == 0)
                ]
                if not boundary_rows.empty:
                    logger.debug(
                        f"[TIMESTAMP TRACE] Found {len(boundary_rows)} rows at exact interval boundaries (minute=0, second=0)"
                    )
                    for j in range(min(3, len(boundary_rows))):
                        logger.debug(
                            f"[TIMESTAMP TRACE] Boundary row {j}: {boundary_rows.iloc[j]['open_time']}"
                        )

        # Filter by the exact time range requested - make sure we include start_time exactly
        # This is critical for proper alignment with interval boundaries
        logger.debug(
            f"Filtering dataframe with time range: {start_time} (inclusive) to {end_time} (inclusive)"
        )

        # Debug: Check inclusion criteria before filtering
        if not concatenated_df.empty:
            would_include_first = concatenated_df["open_time"].min() >= start_time
            would_include_last = concatenated_df["open_time"].max() <= end_time
            logger.debug(
                f"[TIMESTAMP TRACE] Would include first timestamp: {would_include_first}"
            )
            logger.debug(
                f"[TIMESTAMP TRACE] Would include last timestamp: {would_include_last}"
            )

            # Check for timestamps exactly at boundaries
            start_boundary_match = (concatenated_df["open_time"] == start_time).any()
            end_boundary_match = (concatenated_df["open_time"] == end_time).any()
            logger.debug(
                f"[TIMESTAMP TRACE] Exact match at start_time: {start_boundary_match}"
            )
            logger.debug(
                f"[TIMESTAMP TRACE] Exact match at end_time: {end_boundary_match}"
            )

        # Perform the filtering
        filtered_df = filter_dataframe_by_time(
            concatenated_df, start_time, end_time, time_column="open_time"
        )

        # Debug: Check what happened during filtering
        if not filtered_df.empty:
            logger.debug(
                f"[TIMESTAMP TRACE] After time filtering: {len(filtered_df)} rows"
            )
            min_filtered = filtered_df["open_time"].min()
            max_filtered = filtered_df["open_time"].max()
            logger.debug(
                f"[TIMESTAMP TRACE] Filtered time range: {min_filtered} to {max_filtered}"
            )

            # Check if we lost the first candle
            if min_filtered > start_time:
                time_diff = (min_filtered - start_time).total_seconds()
                logger.debug(
                    f"[TIMESTAMP TRACE] First candle starts {time_diff} seconds after requested start_time"
                )
                logger.debug(
                    f"[TIMESTAMP TRACE] This suggests the first candle may have been dropped during filtering"
                )

            # Log first few rows after filtering
            for i in range(min(3, len(filtered_df))):
                logger.debug(
                    f"[TIMESTAMP TRACE] After filtering row {i}: open_time={filtered_df['open_time'].iloc[i]}, close={filtered_df.iloc[i, filtered_df.columns.get_loc('close')]}"
                )
        else:
            logger.debug(
                "[TIMESTAMP TRACE] Filtered DataFrame is empty - all rows were excluded"
            )

            # Log filtering criteria again for clarity
            logger.debug(
                f"[TIMESTAMP TRACE] Filtering criteria used: {start_time} <= open_time <= {end_time}"
            )

        # Log filtering results for debugging
        if not filtered_df.empty:
            logger.debug(f"Filtered dataframe contains {len(filtered_df)} rows")
            logger.debug(
                f"First timestamp after filtering: {filtered_df['open_time'].min()}"
            )
            logger.debug(
                f"Last timestamp after filtering: {filtered_df['open_time'].max()}"
            )
        else:
            logger.warning(
                "Filtered dataframe is empty - no data within requested time range"
            )

        # Use gap_detector to find gaps
        # Convert the interval string to Interval enum for proper gap detection
        try:
            interval_obj = next(
                (i for i in Interval if i.value == self._interval_str), None
            )
            if interval_obj is None:
                interval_obj = Interval.MINUTE_1
                logger.warning(
                    f"Could not find interval {self._interval_str}, using MINUTE_1 as default for gap detection"
                )
        except Exception as e:
            logger.warning(f"Error parsing interval for gap detection: {e}")
            interval_obj = Interval.MINUTE_1

        logger.debug(f"Using interval {interval_obj.value} for gap detection")

        # Detect gaps in the data using the standardized gap_detector
        # Only enforce min span requirement if we're querying a longer timeframe
        time_span_days = (end_time - start_time).total_seconds() / 86400
        min_span_required = time_span_days > 1  # Only for multi-day spans

        if filtered_df.empty:
            gaps = []
            logger.debug("Skipping gap detection for empty dataframe")
        else:
            try:
                if "open_time" in filtered_df.columns:
                    # Temporarily set index to open_time for gap detection
                    df_with_index = filtered_df.set_index("open_time")
                    gaps, stats = detect_gaps(
                        df_with_index,
                        interval_obj,
                        enforce_min_span=min_span_required,
                    )
                else:
                    gaps, stats = detect_gaps(
                        filtered_df,
                        interval_obj,
                        enforce_min_span=min_span_required,
                    )

                # Log the gaps and stats
                if gaps:
                    logger.debug(f"Detected {len(gaps)} gaps in Vision data")
                    logger.debug(f"Gap stats: {stats}")
                    for gap in gaps:
                        logger.debug(
                            f"Gap: {gap.start_time} to {gap.end_time} ({gap.duration.total_seconds()}s)"
                        )
                else:
                    logger.debug("No gaps detected in Vision data")

            except Exception as e:
                logger.error(f"Error detecting gaps: {e}")
                gaps = []

        # Check for day boundary gaps (gaps at midnight)
        boundary_gaps = [
            gap
            for gap in gaps
            if (
                gap.start_time.hour == 0
                and gap.start_time.minute == 0
                and gap.start_time.second == 0
            )
            or (
                gap.end_time.hour == 0
                and gap.end_time.minute == 0
                and gap.end_time.second == 0
            )
        ]

        # Try to fill day boundary gaps using REST API
        if boundary_gaps:
            logger.debug(
                f"Detected {len(boundary_gaps)} day boundary gaps. Attempting to fill with REST API data."
            )
            filled_df = self._fill_boundary_gaps_with_rest(filtered_df, boundary_gaps)
            if filled_df is not None:
                filtered_df = filled_df
                logger.debug(
                    f"Successfully filled boundary gaps. New row count: {len(filtered_df)}"
                )

        # Store original timestamp for reference
        if (
            "open_time" in filtered_df.columns
            and "original_timestamp" not in filtered_df.columns
        ):
            filtered_df["original_timestamp"] = filtered_df["open_time"]
            logger.debug("Preserved original open_time timestamp for reference")

        # Log timestamp semantics for debugging
        logger.debug(
            "Creating TimestampedDataFrame with timestamps representing the START of each candle period"
        )

        # First check if we should be using open_time_us as index
        if (
            "open_time_us" not in filtered_df.columns
            and "open_time" in filtered_df.columns
        ):
            logger.debug(
                f"[TIMESTAMP TRACE] Creating TimestampedDataFrame using open_time as index"
            )
            if not filtered_df.empty:
                for i in range(min(3, len(filtered_df))):
                    logger.debug(
                        f"[TIMESTAMP TRACE] Before index creation row {i}: open_time={filtered_df['open_time'].iloc[i]}, close={filtered_df.iloc[i, filtered_df.columns.get_loc('close')]}"
                    )

            # Create a copy to maintain the original dataframe
            df_for_index = filtered_df.copy()

            # IMPORTANT: When setting open_time as index, we're preserving the semantic meaning
            # that open_time represents the BEGINNING of the candle period for all interval types
            df_for_index = df_for_index.set_index("open_time")

            # Log the first and last timestamps for debugging
            if not df_for_index.empty:
                logger.debug(
                    f"First index timestamp: {df_for_index.index[0]} (represents BEGINNING of candle)"
                )
                logger.debug(
                    f"Last index timestamp: {df_for_index.index[-1]} (represents BEGINNING of candle)"
                )

                # Debug: Log after setting index
                for i in range(min(3, len(df_for_index))):
                    logger.debug(
                        f"[TIMESTAMP TRACE] After index creation row {i}: index={df_for_index.index[i]}, close={df_for_index.iloc[i]['close']}"
                    )

            # Create TimestampedDataFrame preserving exact timestamps and their semantic meaning
            timestamped_df = TimestampedDataFrame(df_for_index)

            # Debug: Final check on TimestampedDataFrame
            if not timestamped_df.empty:
                logger.debug(
                    f"[TIMESTAMP TRACE] Final TimestampedDataFrame has {len(timestamped_df)} rows"
                )
                logger.debug(
                    f"[TIMESTAMP TRACE] Final index (first): {timestamped_df.index[0]}"
                )
                logger.debug(
                    f"[TIMESTAMP TRACE] Final index (last): {timestamped_df.index[-1]}"
                )

            return timestamped_df
        elif "open_time_us" in filtered_df.columns:
            # If open_time_us exists, use it as index
            df_for_index = filtered_df.copy()

            if "open_time" in df_for_index.columns:
                # Avoid ambiguity by removing open_time column before setting index
                df_for_index = df_for_index.drop(columns=["open_time"])

            # Set open_time_us as index
            df_for_index = df_for_index.set_index("open_time_us")
            logger.debug(
                f"Set index to open_time_us with first value {df_for_index.index[0] if not df_for_index.empty else 'N/A'}"
            )

            return TimestampedDataFrame(df_for_index)
        else:
            # If neither column exists, just return the filtered dataframe
            # TimestampedDataFrame initialization will handle validation
            logger.debug("No timestamp column found for index, returning as is")
            return TimestampedDataFrame(filtered_df)

    def _fill_boundary_gaps_with_rest(
        self, df: pd.DataFrame, boundary_gaps: List[Gap]
    ) -> Optional[pd.DataFrame]:
        """Fill day boundary gaps using REST API data.

        This method is used to fill specific gaps that occur at day boundaries
        by fetching the missing data directly from the REST API.

        Args:
            df: DataFrame with Vision API data that has gaps
            boundary_gaps: List of Gap objects representing day boundary gaps

        Returns:
            DataFrame with gaps filled, or None if filling failed
        """
        if not boundary_gaps:
            return df

        # Import RestDataClient here to avoid circular import
        from core.sync.rest_data_client import RestDataClient

        try:
            # Create a REST client with the same parameters
            rest_client = RestDataClient(
                market_type=self.market_type,
                symbol=self._symbol,
                interval=self.interval_obj,
            )

            # Create a list to hold the gap data we'll fetch
            gap_dfs = []
            gap_dfs.append(df)

            # For each gap, fetch the specific missing data
            for gap in boundary_gaps:
                # Add a small buffer around the gap to ensure we get the needed data
                # Use 50% of the interval duration as buffer
                interval_seconds = self.interval_obj.to_seconds()
                buffer_seconds = interval_seconds * 0.5

                # Fetch a bit before and after the actual gap to ensure we get the needed data
                gap_start = gap.start_time - timedelta(seconds=buffer_seconds)
                gap_end = gap.end_time + timedelta(seconds=buffer_seconds)

                logger.debug(
                    f"Fetching gap data from REST API: {gap_start} to {gap_end} "
                    f"(to fill missing data)"
                )

                # Fetch the gap data using REST API
                gap_data = rest_client.fetch(
                    self._symbol,
                    self.interval_obj.value,
                    start_time=gap_start,
                    end_time=gap_end,
                )

                if not gap_data.empty:
                    # Check if we got data around midnight
                    expected_midnight = (
                        gap.start_time + (gap.end_time - gap.start_time) / 2
                    )
                    midnight_time = datetime(
                        expected_midnight.year,
                        expected_midnight.month,
                        expected_midnight.day,
                        0,
                        0,
                        0,
                        tzinfo=expected_midnight.tzinfo,
                    )

                    # Look for records near midnight
                    midnight_records = gap_data[
                        (gap_data["open_time"] - midnight_time).abs()
                        < timedelta(seconds=interval_seconds)
                    ]

                    if not midnight_records.empty:
                        logger.debug(
                            f"Found {len(midnight_records)} records near midnight in REST API data"
                        )
                    else:
                        logger.debug("No midnight records found in REST API data")

                    gap_dfs.append(gap_data)
                else:
                    logger.warning(f"No data retrieved from REST API for gap")

            # If we have gap data, merge it with the original data
            if len(gap_dfs) > 1:  # More than just the original df
                # Concatenate all dataframes and remove duplicates
                merged_df = pd.concat(gap_dfs, ignore_index=True)
                merged_df = merged_df.drop_duplicates(
                    subset=["open_time"], keep="first"
                )
                merged_df = merged_df.sort_values("open_time").reset_index(drop=True)
                return merged_df

            # If we didn't add any gap data, return the original
            return df
        except Exception as e:
            logger.error(f"Error filling boundary gaps with REST API: {e}")
            return None

    def fetch(
        self,
        symbol: str,
        interval: str,
        start_time: datetime,
        end_time: datetime,
        **kwargs,
    ) -> pd.DataFrame:
        """Fetch data for a specific time range from the Binance Vision API.

        This method implements the DataClientInterface fetch method.
        It downloads data from Binance Vision API using daily data files and
        handles date boundaries correctly.

        Args:
            symbol: Trading pair symbol (e.g., "BTCUSDT")
            interval: Time interval string (e.g., "1m", "1h")
            start_time: Start time for data retrieval (timezone-aware datetime)
            end_time: End time for data retrieval (timezone-aware datetime)
            **kwargs: Additional parameters (unused, for interface compatibility)

        Returns:
            DataFrame with data, where open_time is both a column and the index name

        Note:
            This client is optimized for historical data. For recent data (< 2 days old),
            use the RestDataClient as Vision API typically has a 24-48 hour delay.

            Timestamps in the returned data preserve their semantic meaning:
            - open_time represents the BEGINNING of each candle period
            - close_time represents the END of each candle period
        """
        # Validate parameters
        if not isinstance(symbol, str) or not symbol:
            logger.warning(
                f"Invalid symbol: {symbol}, using client symbol {self._symbol}"
            )
            symbol = self._symbol
        elif symbol != self._symbol:
            logger.warning(
                f"Symbol mismatch: requested {symbol}, client configured for {self._symbol}. "
                f"Using client configuration."
            )
            # Continue with the client's configured symbol

        if not isinstance(interval, str) or not interval:
            logger.warning(
                f"Invalid interval: {interval}, using client interval {self._interval_str}"
            )
            interval = self._interval_str
        elif interval != self._interval_str:
            logger.warning(
                f"Interval mismatch: requested {interval}, client configured for {self._interval_str}. "
                f"Using client configuration."
            )
            # Continue with the client's configured interval

        if not isinstance(start_time, datetime) or not isinstance(end_time, datetime):
            raise ValueError("Start time and end time must be datetime objects")

        if start_time >= end_time:
            raise ValueError(
                f"Start time {start_time} must be before end time {end_time}"
            )

        try:
            # Enforce consistent timezone for time boundaries
            start_time = start_time.astimezone(timezone.utc)
            end_time = end_time.astimezone(timezone.utc)

            # Calculate date range
            delta_days = (end_time - start_time).days + 1
            logger.debug(
                f"Requested date range spans {delta_days} days from {start_time} to {end_time}"
            )

            # Log if it's a large request
            if delta_days > 90:
                logger.info(
                    f"Processing a large date range of {delta_days} days with parallel downloads."
                )

            # Download data
            try:
                logger.debug(f"Calling _download_data from {start_time} to {end_time}")
                timestamped_df = self._download_data(start_time, end_time)

                # Use the centralized utility to ensure open_time is properly handled
                # Convert to standard pandas DataFrame first
                if hasattr(timestamped_df, "to_pandas"):
                    df = timestamped_df.to_pandas()
                    logger.debug(
                        f"Converted TimestampedDataFrame to DataFrame using to_pandas()"
                    )
                else:
                    df = pd.DataFrame(timestamped_df)
                    logger.debug(
                        f"Converted TimestampedDataFrame using pd.DataFrame constructor"
                    )

                # Ensure open_time is properly handled
                df = ensure_open_time_as_column(df)

                logger.debug(f"Final DataFrame columns: {list(df.columns)}")
                logger.debug(f"Final DataFrame has {len(df)} rows")

                return df
            except Exception as e:
                logger.error(f"Error in _download_data: {e}")
                import traceback

                logger.error(f"Traceback: {traceback.format_exc()}")
                raise
        except Exception as e:
            logger.error(f"Error fetching data: {e}")
            return self.create_empty_dataframe()

    @staticmethod
    def fetch_multiple(
        symbols: List[str],
        start_time: datetime,
        end_time: datetime,
        interval: str = "1m",
        market_type: Union[str, MarketType] = MarketType.SPOT,
        max_workers: Optional[int] = None,
    ) -> Dict[str, TimestampedDataFrame]:
        """Fetch data for multiple symbols in parallel.

        Args:
            symbols: List of trading symbols to fetch data for
            start_time: Start time for data
            end_time: End time for data
            interval: Kline interval e.g. '1s', '1m'
            market_type: Market type (SPOT, FUTURES_USDT, FUTURES_COIN) or string
            max_workers: Maximum number of parallel workers (defaults to min(MAXIMUM_CONCURRENT_DOWNLOADS, len(symbols)))

        Returns:
            Dictionary mapping symbols to their respective DataFrames
        """
        if not symbols:
            logger.warning("No symbols provided to fetch_multiple")
            return {}

        # Calculate effective number of workers
        if max_workers is None:
            max_workers = min(MAXIMUM_CONCURRENT_DOWNLOADS, len(symbols))
        else:
            max_workers = min(max_workers, MAXIMUM_CONCURRENT_DOWNLOADS, len(symbols))

        # Calculate date range for logging
        delta_days = (end_time - start_time).days + 1

        # Log large requests but don't limit them
        if delta_days > 90:
            logger.info(
                f"Processing a large date range of {delta_days} days for {len(symbols)} symbols. This is supported with parallel downloads."
            )

        logger.info(
            f"Fetching data for {len(symbols)} symbols using {max_workers} parallel workers"
        )

        results: Dict[str, TimestampedDataFrame] = {}

        # Define worker function to download data for a single symbol
        def download_worker(symbol: str) -> Tuple[str, TimestampedDataFrame]:
            try:
                with VisionDataClient(
                    symbol=symbol, interval=interval, market_type=market_type
                ) as client:
                    df = client.fetch(symbol, interval, start_time, end_time)
                return symbol, df
            except Exception as e:
                logger.error(f"Error fetching data for {symbol}: {e}")
                # Return empty dataframe on error
                client = VisionDataClient(
                    symbol=symbol, interval=interval, market_type=market_type
                )
                empty_df = client.create_empty_dataframe()
                client.close()
                return symbol, empty_df

        # Use ThreadPoolExecutor to parallelize downloads across symbols
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_symbol = {
                executor.submit(download_worker, symbol): symbol for symbol in symbols
            }

            # Process results as they complete
            for i, future in enumerate(as_completed(future_to_symbol)):
                symbol = future_to_symbol[future]
                try:
                    symbol_result, df = future.result()
                    results[symbol_result] = df
                    logger.info(
                        f"Completed download for {symbol} ({i+1}/{len(symbols)}): {len(df)} records"
                    )
                except Exception as e:
                    logger.error(f"Error processing result for {symbol}: {e}")
                    # Create empty dataframe for failed symbols
                    client = VisionDataClient(
                        symbol=symbol, interval=interval, market_type=market_type
                    )
                    results[symbol] = client.create_empty_dataframe()
                    client.close()

        return results
