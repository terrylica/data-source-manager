#!/usr/bin/env python
"""Integration tests for DataSourceManager cache optimization."""

import unittest
from pathlib import Path
from unittest.mock import MagicMock, patch

import pandas as pd
import pendulum

from core.sync.data_source_manager import DataSourceManager
from utils.config import FEATURE_FLAGS
from utils.for_core.dsm_cache_utils import get_from_cache
from utils.market_constraints import ChartType, DataProvider, Interval, MarketType


class TestDsmCacheUtils(unittest.TestCase):
    """Unit tests for the cache optimization functionality."""

    def setUp(self):
        """Set up test fixtures."""
        self.symbol = "BTCUSDT"
        self.provider = "BINANCE"
        self.chart_type = "KLINES"
        self.market_type = "SPOT"
        self.interval = Interval("5m")

        # Create a mock cache manager
        self.cache_manager = MagicMock()

        # Time range for the tests
        self.start_time = pendulum.datetime(2025, 4, 13, 15, 35, 0, tz="UTC")
        self.end_time = pendulum.datetime(2025, 4, 14, 15, 30, 0, tz="UTC")

    def test_cache_optimization_incomplete_days(self):
        """
        TEST_CASE_ID:CACHE-OPT-001 - Critical Optimization Test (Unit Test)

        Test the optimization that prevents unnecessary API calls when:
        1. A day is incomplete (less than 90% of total records)
        2. BUT all required records for the specified time range are present

        This test validates the business requirement that partial days shouldn't be
        refetched when they already contain all data needed for the requested time range.
        """
        # Create test data for two days
        # Day 1: 50% complete but has all records for the requested time range
        day1 = pendulum.datetime(2025, 4, 13, 0, 0, 0, tz="UTC")
        day1_records = []

        # Create only records from 12:00 to 23:55 (half a day - incomplete)
        for i in range(12 * 12):  # 12 hours * 12 5-min intervals
            timestamp = day1.add(hours=12).add(minutes=i * 5)
            day1_records.append(
                {
                    "open_time": timestamp,
                    "close_time": timestamp.add(minutes=5).subtract(microseconds=1),
                    "open": 100.0,
                    "high": 101.0,
                    "low": 99.0,
                    "close": 100.5,
                    "volume": 100.0,
                    "quote_asset_volume": 10050.0,
                    "count": 100,
                    "taker_buy_volume": 50.0,
                    "taker_buy_quote_volume": 5025.0,
                    "ignore": 0,
                }
            )

        day1_df = pd.DataFrame(day1_records)

        # Day 2: 50% complete but has all records for the requested time range
        day2 = pendulum.datetime(2025, 4, 14, 0, 0, 0, tz="UTC")
        day2_records = []

        # Create only records from 00:00 to 15:55
        for i in range(16 * 12):  # 16 hours * 12 5-min intervals
            timestamp = day2.add(minutes=i * 5)
            day2_records.append(
                {
                    "open_time": timestamp,
                    "close_time": timestamp.add(minutes=5).subtract(microseconds=1),
                    "open": 100.0,
                    "high": 101.0,
                    "low": 99.0,
                    "close": 100.5,
                    "volume": 100.0,
                    "quote_asset_volume": 10050.0,
                    "count": 100,
                    "taker_buy_volume": 50.0,
                    "taker_buy_quote_volume": 5025.0,
                    "ignore": 0,
                }
            )

        day2_df = pd.DataFrame(day2_records)

        # Setup mock returns
        self.cache_manager.load_from_cache.side_effect = [day1_df, day2_df]

        # Make sure the feature flag is enabled for this test
        original_flag_value = FEATURE_FLAGS.get("OPTIMIZE_CACHE_PARTIAL_DAYS", True)
        FEATURE_FLAGS["OPTIMIZE_CACHE_PARTIAL_DAYS"] = True

        try:
            # Request data from 13th 15:35 to 14th 15:30 (our test case)
            # Day 1 should have all records needed for 15:35-23:55
            # Day 2 should have all records needed for 00:00-15:30
            df, missing_ranges = get_from_cache(
                symbol=self.symbol,
                start_time=self.start_time,
                end_time=self.end_time,
                interval=self.interval,
                cache_manager=self.cache_manager,
                provider=self.provider,
                chart_type=self.chart_type,
                market_type=self.market_type,
            )

            # Verify results
            # 1. No missing ranges should be reported
            self.assertEqual(
                len(missing_ranges),
                0,
                "Expected no missing ranges when cache has all required records",
            )

            # 2. Both days should be retrieved from cache even though they're incomplete
            self.cache_manager.load_from_cache.assert_any_call(
                symbol=self.symbol,
                interval=self.interval.value,
                date=day1,
                provider=self.provider,
                chart_type=self.chart_type,
                market_type=self.market_type,
            )

            self.cache_manager.load_from_cache.assert_any_call(
                symbol=self.symbol,
                interval=self.interval.value,
                date=day2,
                provider=self.provider,
                chart_type=self.chart_type,
                market_type=self.market_type,
            )

            # 3. Verify we got a complete dataset back
            # The number of 5-min intervals between 2025-04-13 15:35 and 2025-04-14 15:30
            expected_intervals = (
                int(
                    (self.end_time - self.start_time).total_seconds()
                    / self.interval.to_seconds()
                )
                + 1
            )
            self.assertEqual(
                len(df),
                expected_intervals,
                f"Expected {expected_intervals} records in the result DataFrame",
            )
        finally:
            # Restore the original feature flag value
            FEATURE_FLAGS["OPTIMIZE_CACHE_PARTIAL_DAYS"] = original_flag_value


class TestDsmCacheOptimization(unittest.TestCase):
    """
    Integration tests for the DataSourceManager cache optimization feature.

    This test suite verifies the end-to-end functionality of the OPTIMIZE_CACHE_PARTIAL_DAYS
    feature flag, ensuring it properly prevents unnecessary API calls when cache has
    all required data for a given time range, even if the days are not complete.
    """

    def setUp(self):
        """Set up test environment."""
        # Create a test cache directory
        self.cache_dir = Path("./test_cache")
        self.cache_dir.mkdir(exist_ok=True, parents=True)

        # Create mock cache manager and clients
        self.cache_manager = MagicMock()
        self.vision_client = MagicMock()
        self.rest_client = MagicMock()

        # Test parameters
        self.symbol = "BTCUSDT"
        self.market_type = MarketType.SPOT
        self.interval = Interval("5m")
        self.provider = DataProvider.BINANCE
        self.chart_type = ChartType.KLINES

        # Time range
        self.start_time = pendulum.datetime(2025, 4, 13, 15, 35, 0, tz="UTC")
        self.end_time = pendulum.datetime(2025, 4, 14, 15, 30, 0, tz="UTC")

        # Save the original feature flag setting
        self.original_flag_value = FEATURE_FLAGS.get(
            "OPTIMIZE_CACHE_PARTIAL_DAYS", True
        )

    def tearDown(self):
        """Clean up after tests."""
        # Remove the test cache directory
        import shutil

        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)

        # Restore the original feature flag value
        FEATURE_FLAGS["OPTIMIZE_CACHE_PARTIAL_DAYS"] = self.original_flag_value

    def test_optimization_enabled(self):
        """Test with optimization enabled - should not make API calls."""
        # Enable the optimization
        FEATURE_FLAGS["OPTIMIZE_CACHE_PARTIAL_DAYS"] = True

        # Mock the API calls to track if they're made
        api_called = {"vision": False, "rest": False}

        def mock_vision_get(*args, **kwargs):
            api_called["vision"] = True
            return pd.DataFrame()  # Empty DataFrame

        def mock_rest_get(*args, **kwargs):
            api_called["rest"] = True
            return pd.DataFrame()  # Empty DataFrame

        # Create a complete dataset for the time range
        all_records = []
        current_time = self.start_time
        while current_time <= self.end_time:
            all_records.append(
                {
                    "open_time": current_time,
                    "close_time": current_time.add(minutes=5).subtract(microseconds=1),
                    "open": 100.0,
                    "high": 101.0,
                    "low": 99.0,
                    "close": 100.5,
                    "volume": 100.0,
                    "quote_asset_volume": 10050.0,
                    "count": 100,
                    "taker_buy_volume": 50.0,
                    "taker_buy_quote_volume": 5025.0,
                    "ignore": 0,
                    "_data_source": "CACHE",
                }
            )
            current_time = current_time.add(minutes=5)
        complete_df = pd.DataFrame(all_records)

        # Create a DSM instance with patched methods
        with patch.object(
            DataSourceManager, "_get_from_cache", return_value=(complete_df, [])
        ), patch.object(
            DataSourceManager, "_fetch_from_vision", side_effect=mock_vision_get
        ), patch.object(
            DataSourceManager, "_fetch_from_rest", side_effect=mock_rest_get
        ):

            # Initialize DSM directly
            dsm = DataSourceManager(
                provider=DataProvider.BINANCE, cache_dir=self.cache_dir
            )

            # Get data for the time range
            df = dsm.get_data(
                symbol=self.symbol,
                start_time=self.start_time,
                end_time=self.end_time,
                interval=self.interval,
                chart_type=self.chart_type,
                include_source_info=True,
            )

            # Verify no API calls were made
            self.assertFalse(
                api_called["vision"], "Vision API should not have been called"
            )
            self.assertFalse(api_called["rest"], "REST API should not have been called")

            # Verify we got the expected number of records
            expected_intervals = (
                int(
                    (self.end_time - self.start_time).total_seconds()
                    / self.interval.to_seconds()
                )
                + 1
            )
            self.assertEqual(
                len(df),
                expected_intervals,
                f"Expected {expected_intervals} records in the result",
            )

            # Verify all records are from cache
            self.assertTrue(
                all(df["_data_source"] == "CACHE"), "All records should be from cache"
            )

    def test_optimization_disabled(self):
        """Test with optimization disabled - should make API calls for incomplete days."""
        # Disable the optimization
        FEATURE_FLAGS["OPTIMIZE_CACHE_PARTIAL_DAYS"] = False

        # Mock the API calls to track if they're made
        api_called = {"vision": False, "rest": False}

        # Create incomplete data
        incomplete_records = []

        # We'll just include some records to simulate partial cache
        current_time = self.start_time
        for _ in range(10):  # Just add 10 records
            incomplete_records.append(
                {
                    "open_time": current_time,
                    "close_time": current_time.add(minutes=5).subtract(microseconds=1),
                    "open": 100.0,
                    "high": 101.0,
                    "low": 99.0,
                    "close": 100.5,
                    "volume": 100.0,
                    "quote_asset_volume": 10050.0,
                    "count": 100,
                    "taker_buy_volume": 50.0,
                    "taker_buy_quote_volume": 5025.0,
                    "ignore": 0,
                    "_data_source": "CACHE",
                }
            )
            current_time = current_time.add(minutes=5)

        incomplete_df = pd.DataFrame(incomplete_records)

        # Create a missing range to force API calls
        missing_ranges = [(self.start_time.add(minutes=50), self.end_time)]

        # Create REST result data
        rest_records = []
        rest_start = self.start_time.add(minutes=50)
        current_time = rest_start
        while current_time <= self.end_time:
            rest_records.append(
                {
                    "open_time": current_time,
                    "close_time": current_time.add(minutes=5).subtract(microseconds=1),
                    "open": 101.0,  # Different value to identify REST data
                    "high": 102.0,
                    "low": 100.0,
                    "close": 101.5,
                    "volume": 110.0,
                    "quote_asset_volume": 11050.0,
                    "count": 110,
                    "taker_buy_volume": 55.0,
                    "taker_buy_quote_volume": 5525.0,
                    "ignore": 0,
                }
            )
            current_time = current_time.add(minutes=5)
        rest_df = pd.DataFrame(rest_records)
        rest_df["_data_source"] = "REST"

        # Skip Vision API completely and go straight to REST
        # Instead of failing with an error, we'll just return an empty DataFrame
        def mock_vision_get(*args, **kwargs):
            api_called["vision"] = True
            return pd.DataFrame()  # Empty DataFrame to trigger REST fallback

        def mock_rest_get(*args, **kwargs):
            api_called["rest"] = True
            return rest_df

        # Create a DSM instance with patched methods
        with patch.object(
            DataSourceManager,
            "_get_from_cache",
            return_value=(incomplete_df, missing_ranges),
        ), patch.object(
            DataSourceManager, "_fetch_from_vision", side_effect=mock_vision_get
        ), patch.object(
            DataSourceManager, "_fetch_from_rest", side_effect=mock_rest_get
        ):

            # Initialize DSM directly
            dsm = DataSourceManager(
                provider=DataProvider.BINANCE, cache_dir=self.cache_dir
            )

            # Get data for the time range
            df = dsm.get_data(
                symbol=self.symbol,
                start_time=self.start_time,
                end_time=self.end_time,
                interval=self.interval,
                chart_type=self.chart_type,
                include_source_info=True,
            )

            # Verify REST API call was made (Vision fails, REST is used)
            self.assertTrue(api_called["rest"], "REST API should have been called")

            # Verify the result contains data from both sources
            self.assertTrue("_data_source" in df.columns, "Missing data source column")
            self.assertTrue("CACHE" in df["_data_source"].values, "Missing cache data")
            self.assertTrue("REST" in df["_data_source"].values, "Missing REST data")


if __name__ == "__main__":
    unittest.main()
