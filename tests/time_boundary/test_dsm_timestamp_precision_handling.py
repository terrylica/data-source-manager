#!/usr/bin/env python
"""Integration tests for DataSourceManager focusing on time formats and precision.

System Under Test (SUT):
- core.data_source_manager.DataSourceManager
- utils.time_alignment (indirectly)
- core.vision_data_client.VisionDataClient (indirectly)
- core.market_data_client.EnhancedRetriever (indirectly)

This test suite verifies the timestamp handling behavior in the DataSourceManager
with a focus on precision and format consistency across different data sources.

Focus areas:
1. Time boundary behaviors and alignment
2. Input format validation and defaults
3. Output format consistency and guarantees
4. Timestamp precision handling
5. Timezone handling and conversions

This is the primary file for all timestamp format and precision related tests.
It defines the core expectations and guarantees about how timestamps are handled.

Related tests:
- For specific boundary edge cases: see test_data_source_manager_boundary.py
- For year boundary specific tests: see test_data_source_manager_year_boundary.py
- For general validation and data tests: see test_data_source_manager_consolidated.py
"""

import pytest
import arrow
import pandas as pd
from datetime import timedelta, timezone, datetime
from typing import Any, cast as type_cast
import numpy as np
import pytest_asyncio

from utils.logger_setup import get_logger
from core.data_source_manager import DataSourceManager
from utils.market_constraints import Interval, MarketType

logger = get_logger(__name__, "INFO", show_path=False)

# Test configuration
TEST_SYMBOL = "BTCUSDT"  # Use BTC for reliable data
TEST_INTERVAL = Interval.SECOND_1  # Focus on 1-second data

# Time constants for tests
HOUR = timedelta(hours=1)
DAY = timedelta(days=1)


@pytest.fixture
def now() -> arrow.Arrow:
    """Get current time for tests."""
    return arrow.utcnow()


def to_arrow(dt: Any) -> arrow.Arrow:
    """Convert various datetime types to Arrow.

    Args:
        dt: Input datetime in any format

    Returns:
        Arrow object

    Raises:
        ValueError: If input is NaT or invalid
    """
    # Handle NaT explicitly for pandas types
    if pd.api.types.is_datetime64_any_dtype(dt) and pd.isna(dt):
        raise ValueError("Cannot convert NaT (Not a Time) to Arrow")

    if isinstance(dt, arrow.Arrow):
        return dt
    if isinstance(dt, datetime):
        return arrow.get(type_cast(datetime, dt))
    if isinstance(dt, pd.Timestamp):
        if dt is pd.NaT:
            raise ValueError("Cannot convert NaT (Not a Time) to Arrow")
        pdt = dt.to_pydatetime()
        return arrow.get(type_cast(datetime, pdt))

    # Convert other types through Timestamp
    try:
        ts = pd.Timestamp(dt)
        if ts is pd.NaT:
            raise ValueError("Cannot convert NaT (Not a Time) to Arrow")
        pdt = ts.to_pydatetime()
        return arrow.get(type_cast(datetime, pdt))
    except (ValueError, TypeError) as e:
        raise ValueError(f"Cannot convert {type(dt)} to Arrow: {str(e)}")


def log_test_motivation(
    test_name: str, motivation: str, expectations: list[str], implications: list[str]
) -> None:
    """Log detailed test motivation and expectations."""
    logger.info("")
    logger.info(
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
    logger.info(f"â•‘ ðŸ§ª TEST CASE: {test_name}")
    logger.info(
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
    logger.info("â•‘ ðŸŽ¯ MOTIVATION:")
    for line in motivation.split("\n"):
        logger.info(f"â•‘   {line.strip()}")

    logger.info("â•‘")
    logger.info("â•‘ âœ… EXPECTATIONS:")
    for i, exp in enumerate(expectations, 1):
        logger.info(f"â•‘   {i}. {exp}")

    logger.info("â•‘")
    logger.info("â•‘ ðŸ’¡ BUSINESS/TECHNICAL IMPLICATIONS:")
    for i, imp in enumerate(implications, 1):
        logger.info(f"â•‘   {i}. {imp}")

    logger.info(
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )


def log_dataframe_info(df: pd.DataFrame, source: str) -> None:
    """Log detailed DataFrame information for analysis."""
    logger.info(
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
    logger.info(f"â•‘ ðŸ“Š DATA ANALYSIS REPORT - {source}")
    logger.info(
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )

    if df.empty:
        logger.warning("â•‘ âš ï¸  DataFrame is empty!")
        logger.info(
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )
        return

    # Basic Information
    logger.info("â•‘ ðŸ“Œ Basic Information:")
    logger.info(f"â•‘   â€¢ ðŸ“‘ Records: {df.shape[0]:,}")
    logger.info(f"â•‘   â€¢ ðŸ“Š Columns: {df.shape[1]}")
    logger.info(f"â•‘   â€¢ ðŸ”‘ Index Type: {type(df.index).__name__}")
    if isinstance(df.index, pd.DatetimeIndex):
        logger.info(f"â•‘   â€¢ ðŸŒ Timezone: {df.index.tz or 'naive'}")
    else:
        logger.info("â•‘   â€¢ ðŸŒ Timezone: N/A (not a DatetimeIndex)")

    # Time Range Analysis
    logger.info("â•‘")
    logger.info("â•‘ â° Time Range Analysis:")
    first_ts = to_arrow(df.index[0])
    last_ts = to_arrow(df.index[-1])
    logger.info(
        f"â•‘   â€¢ ðŸ”µ First Record: {first_ts.format('YYYY-MM-DD HH:mm:ss.SSSSSS')} UTC"
    )
    logger.info(
        f"â•‘   â€¢ ðŸ”´ Last Record: {last_ts.format('YYYY-MM-DD HH:mm:ss.SSSSSS')} UTC"
    )
    logger.info(f"â•‘   â€¢ âŒ› Total Duration: {last_ts - first_ts}")
    logger.info(f"â•‘   â€¢ ðŸŽ¯ Record Count: {len(df):,}")

    # Data Types and Format
    logger.info("â•‘")
    logger.info("â•‘ ðŸ”§ Column Data Types and Sample Values:")
    for col in df.columns:
        sample_val = df[col].iloc[0]
        logger.info(f"â•‘   â€¢ {col}: {df[col].dtype} (e.g., {sample_val})")

    logger.info(
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )


@pytest_asyncio.fixture
async def manager():
    """Create DataSourceManager instance."""
    async with DataSourceManager(market_type=MarketType.SPOT) as mgr:
        yield mgr


@pytest.mark.real
@pytest.mark.asyncio
async def test_time_boundary_alignment(manager: DataSourceManager, now: arrow.Arrow):
    """Test how DataSourceManager aligns time boundaries for different inputs."""
    log_test_motivation(
        "Time Boundary Alignment",
        "Understanding how DataSourceManager handles various time boundary inputs "
        "and what alignment rules are applied to start/end times.",
        expectations=[
            "Microsecond precision in timestamps",
            "Proper handling of non-aligned timestamps",
            "Consistent boundary alignment rules",
            "Timezone-aware index in UTC",
        ],
        implications=[
            "Defines timestamp precision requirements",
            "Clarifies boundary alignment behavior",
            "Documents timezone handling",
            "Establishes input format standards",
        ],
    )

    # Test cases with different time alignments (using historical dates)
    base_time = now.shift(days=-2)
    test_cases = [
        # Case 1: Clean second boundaries
        (base_time.floor("second"), base_time.floor("second").shift(seconds=10)),
        # Case 2: Millisecond precision
        (
            base_time.shift(microseconds=500000),
            base_time.shift(seconds=10, microseconds=750000),
        ),
        # Case 3: Microsecond precision
        (
            base_time.shift(microseconds=123456),
            base_time.shift(seconds=10, microseconds=987654),
        ),
    ]

    for start_time, end_time in test_cases:
        logger.info(f"Testing time range: {start_time} to {end_time}")

        df = await manager.get_data(
            symbol=TEST_SYMBOL,
            interval=TEST_INTERVAL,
            start_time=start_time.datetime,
            end_time=end_time.datetime,
            use_cache=False,
        )

        log_dataframe_info(
            df,
            f"Alignment Test ({start_time.format('ss.SSSSSS')} to {end_time.format('ss.SSSSSS')})",
        )

        # Skip validation for empty dataframes
        if df.empty:
            logger.warning("Empty DataFrame returned - skipping validation")
            continue

        # Verify index properties
        assert isinstance(df.index, pd.DatetimeIndex), "Index must be DatetimeIndex"
        assert df.index.tz == timezone.utc, "Index must be UTC timezone-aware"

        # Check timestamp precision
        index_microseconds = df.index.astype(np.int64) % 1_000_000
        assert np.all(
            index_microseconds == 0
        ), "Timestamps should be aligned to seconds"


@pytest.mark.real
@pytest.mark.asyncio
async def test_input_format_handling(manager: DataSourceManager, now: arrow.Arrow):
    """Test DataSourceManager's handling of various input formats."""
    log_test_motivation(
        "Input Format Handling",
        "Verify that the DataSourceManager properly handles different input formats "
        "for time parameters and consistently produces well-formatted output.",
        expectations=[
            "Timezone-naive and aware input acceptance",
            "Various datetime formats properly normalized",
            "Consistent UTC timezone output regardless of input",
            "Proper handling of boundary cases (exact second, millisecond, microsecond precision)",
        ],
        implications=[
            "Establishes input format flexibility",
            "Documents format normalization behavior",
            "Prevents timezone-related issues",
            "Defines input validation standards",
        ],
    )

    # Test cases with different input formats
    test_cases = [
        # Case 1: Naive datetime
        (
            datetime.now() - timedelta(days=2),
            datetime.now() - timedelta(days=2) + timedelta(minutes=5),
            "Naive datetime",
        ),
        # Case 2: Aware datetime (UTC)
        (
            datetime.now(timezone.utc) - timedelta(days=2),
            datetime.now(timezone.utc) - timedelta(days=2) + timedelta(minutes=5),
            "Aware datetime (UTC)",
        ),
    ]

    for start_time, end_time, case_name in test_cases:
        logger.info(f"Testing input format: {case_name}")
        logger.info(f"Input - Start: {start_time}, End: {end_time}")

        df = await manager.get_data(
            symbol=TEST_SYMBOL,
            interval=TEST_INTERVAL,
            start_time=start_time,
            end_time=end_time,
            use_cache=False,
        )

        log_dataframe_info(df, f"Input Format Test ({case_name})")

        # Skip validation for empty dataframes
        if df.empty:
            logger.warning(
                f"Empty DataFrame returned for {case_name} - skipping validation"
            )
            continue

        # Verify core structure with any format
        assert isinstance(df.index, pd.DatetimeIndex), "Index must be DatetimeIndex"
        assert df.index.tz == timezone.utc, "Index must have UTC timezone"

        # Check essential properties
        assert not df.empty, "DataFrame should not be empty"
        assert pd.api.types.is_datetime64_dtype(
            df["close_time"]
        ), "close_time must be datetime64"
        assert df["trades"].dtype == np.int64, "trades should be int64"

        # Check if we have a reasonable number of records
        expected_minutes = 5  # Time range is 5 minutes
        expected_records = expected_minutes * 60  # For 1-second interval
        assert len(df) > 0, "Should have at least some records"

        # More lenient check on record count since we might not get exactly expected records
        # due to time boundary handling changes
        assert (
            0 < len(df) <= expected_records * 1.1
        ), f"Record count {len(df)} should be reasonable"


@pytest.mark.real
@pytest.mark.asyncio
async def test_output_format_guarantees(manager: DataSourceManager, now: arrow.Arrow):
    """
    Test output format guarantees of DataSourceManager.

    After time alignment revamp, this test handles the possibility of empty DataFrames,
    which may occur due to the more stringent time boundary handling but still verifies
    the basic structure and format guarantees.
    """
    log_test_motivation(
        "Output Format Guarantees",
        "Verify that the output format of DataSourceManager is consistent across all operations, "
        "to provide a predictable interface for downstream components.",
        expectations=[
            "DatetimeIndex with UTC timezone",
            "Consistent column presence and types",
            "No duplicate indices",
            "Regular interval timestamps",
            "Close time present and properly formatted",
        ],
        implications=[
            "Establishes output format contract",
            "Enables reliable downstream processing",
            "Prevents timezone-related bugs",
            "Ensures predictable DataFrame structure",
        ],
    )

    # Test with historical data to ensure consistency
    start_time = now.shift(days=-2).floor("hour").datetime
    end_time = start_time + timedelta(minutes=5)

    logger.info(f"Testing data from {start_time} to {end_time}")
    df = await manager.get_data(
        symbol=TEST_SYMBOL,
        interval=TEST_INTERVAL,
        start_time=start_time,
        end_time=end_time,
        use_cache=False,
    )

    log_dataframe_info(df, "Output Format Test")

    # After the time alignment revamp, we might get empty dataframes
    # In such cases, validate the basic structure but skip detailed content checks
    if df.empty:
        logger.warning(
            "Empty DataFrame returned - this may be acceptable after time alignment revamp"
        )
        # Validate the basic structure for empty dataframes
        assert isinstance(
            df, pd.DataFrame
        ), "Result should be a DataFrame even if empty"

        # Verify essential columns exist (using more flexible check)
        essential_columns = ["open", "high", "low", "close", "volume", "close_time"]
        for col in essential_columns:
            assert (
                col in df.columns
            ), f"Column {col} should exist in the empty DataFrame"

        # Verify that we have a columns that correspond to the expected column types
        # (though the exact names may be different)
        assert any(
            "quote" in col.lower() for col in df.columns
        ), "Should have a quote volume column"
        assert any(
            "trade" in col.lower() for col in df.columns
        ), "Should have a trades column"
        assert any(
            "taker" in col.lower() and "buy" in col.lower() for col in df.columns
        ), "Should have taker buy columns"

        return

    # Validate core structure
    assert isinstance(df.index, pd.DatetimeIndex), "Index must be DatetimeIndex"
    assert df.index.tz == timezone.utc, "Index must have UTC timezone"
    assert df.index.is_monotonic_increasing, "Index must be monotonically increasing"
    assert not df.index.has_duplicates, "Index must not have duplicates"

    # Validate column types with more flexible approach
    # Special case for close_time which is now a datetime
    assert "close_time" in df.columns, "close_time column must be present"
    assert str(df["close_time"].dtype).startswith(
        "datetime64"
    ), "close_time should be datetime64"

    # Check numeric columns
    numeric_columns = ["open", "high", "low", "close", "volume"]
    for col in numeric_columns:
        assert col in df.columns, f"Column {col} must be present"
        assert pd.api.types.is_numeric_dtype(
            df[col].dtype
        ), f"Column {col} must be numeric"

    # Validate values are sensible
    assert (df["high"] >= df["low"]).all(), "High must be >= Low"
    assert (df["high"] >= df["open"]).all(), "High must be >= Open"
    assert (df["high"] >= df["close"]).all(), "High must be >= Close"
    assert (df["volume"] >= 0).all(), "Volume must be non-negative"


@pytest.mark.real
@pytest.mark.asyncio
async def test_timestamp_precision_handling(
    manager: DataSourceManager, now: arrow.Arrow
):
    """
    Test timestamp precision handling in DataSourceManager output.

    After time alignment revamp, this test handles the possibility of empty DataFrames,
    which may occur due to the more stringent time boundary handling but still verifies
    the basic format expectations.
    """
    log_test_motivation(
        "Timestamp Precision Handling",
        "Verify that timestamp precision is maintained and handled consistently "
        "for both index and timestamp columns.",
        expectations=[
            "Microsecond precision in DatetimeIndex",
            "Consistent datetime format for close_time",
            "Timezone-aware timestamps",
            "All timestamps with proper units and precision",
        ],
        implications=[
            "Ensures accurate time comparison",
            "Prevents timestamp rounding issues",
            "Guarantees precision for downstream analysis",
            "Provides basis for time-sensitive operations",
        ],
    )

    # Test with a short time window starting at a non-aligned time
    # (including microseconds to test precision handling)
    start_time = now.shift(days=-1).replace(microsecond=123456).datetime
    end_time = start_time + timedelta(minutes=1)

    logger.info(f"Testing timestamp precision with range: {start_time} to {end_time}")
    df = await manager.get_data(
        symbol=TEST_SYMBOL,
        interval=TEST_INTERVAL,
        start_time=start_time,
        end_time=end_time,
        use_cache=False,
    )

    log_dataframe_info(df, "Timestamp Precision Test")

    # After the time alignment revamp, we might get empty dataframes
    # In such cases, validate the basic structure but skip detailed content checks
    if df.empty:
        logger.warning(
            "Empty DataFrame returned - this may be acceptable after time alignment revamp"
        )
        # Validate the basic structure for empty dataframes
        assert isinstance(
            df, pd.DataFrame
        ), "Result should be a DataFrame even if empty"

        # Verify essential columns exist (using more flexible check)
        essential_columns = ["open", "high", "low", "close", "volume", "close_time"]
        for col in essential_columns:
            assert (
                col in df.columns
            ), f"Column {col} should exist in the empty DataFrame"

        # Verify that we have a columns that correspond to the expected column types
        # (though the exact names may be different)
        assert any(
            "quote" in col.lower() for col in df.columns
        ), "Should have a quote volume column"
        assert any(
            "trade" in col.lower() for col in df.columns
        ), "Should have a trades column"
        assert any(
            "taker" in col.lower() and "buy" in col.lower() for col in df.columns
        ), "Should have taker buy columns"

        return

    # Verify timestamp properties
    assert isinstance(df.index, pd.DatetimeIndex), "Index must be DatetimeIndex"
    assert df.index.tz == timezone.utc, "Index must have UTC timezone"

    # Verify close_time is datetime type
    assert pd.api.types.is_datetime64_dtype(
        df["close_time"]
    ), "close_time must be datetime64"

    # Check timestamp relationship: close_time should be very close to index + 1 second
    # The exact implementation might vary slightly, but the relationship should be maintained
    for idx, row in df.iterrows():
        time_diff = (row["close_time"] - idx).total_seconds()
        assert (
            0.9 <= time_diff <= 1.1
        ), f"close_time should be ~1 second after index: {time_diff}"

    # Verify all timestamps have proper timezone
    assert all(
        ts.tzinfo is not None for ts in df.index
    ), "All index timestamps must have timezone"
    assert all(
        ts.tzinfo is not None for ts in df["close_time"]
    ), "All close_time values must have timezone"


@pytest.mark.real
@pytest.mark.asyncio
async def test_data_point_relationships(manager: DataSourceManager, now: arrow.Arrow):
    """Test relationships between data points."""
    log_test_motivation(
        "Data Point Relationships",
        "Verify that relationships between different data points (open, close, high, low, "
        "volume, etc.) follow expected patterns and maintain logical consistency.",
        expectations=[
            "Price relationships (high â‰¥ open, high â‰¥ close, high â‰¥ low)",
            "Volume consistency (volume > 0 when trades > 0)",
            "Quote asset volume â‰ˆ avg_price * volume",
            "Taker volumes are subsets of total volumes",
        ],
        implications=[
            "Validates data quality",
            "Ensures logical consistency",
            "Provides guarantees for analysis",
            "Documents expected relationships",
        ],
    )

    # Use a well-traded period for reliable data
    start_time = now.shift(days=-1).floor("hour").datetime
    end_time = start_time + timedelta(minutes=30)

    logger.info(f"Testing data point relationships from {start_time} to {end_time}")
    df = await manager.get_data(
        symbol=TEST_SYMBOL,
        interval=TEST_INTERVAL,
        start_time=start_time,
        end_time=end_time,
        use_cache=False,
    )

    log_dataframe_info(df, "Data Relationships Test")

    # Skip empty DataFrame
    if df.empty:
        logger.warning("Empty DataFrame returned, skipping relationship tests")
        return

    # Price relationships
    assert (df["high"] >= df["open"]).all(), "High should be â‰¥ Open"
    assert (df["high"] >= df["close"]).all(), "High should be â‰¥ Close"
    assert (df["high"] >= df["low"]).all(), "High should be â‰¥ Low"
    assert (df["low"] <= df["open"]).all(), "Low should be â‰¤ Open"
    assert (df["low"] <= df["close"]).all(), "Low should be â‰¤ Close"

    # Volume relationships
    assert (df["volume"] >= 0).all(), "Volume should be â‰¥ 0"
    assert (df["quote_volume"] >= 0).all(), "Quote volume should be â‰¥ 0"
    assert (
        df.loc[df["trades"] > 0, "volume"] > 0
    ).all(), "Volume should be > 0 when trades > 0"

    # Taker buy volume relationships (using correct column names)
    assert (
        df["taker_buy_base_volume"] <= df["volume"]
    ).all(), "Taker buy base volume should be â‰¤ total volume"
    assert (
        df["taker_buy_quote_volume"] <= df["quote_volume"]
    ).all(), "Taker buy quote volume should be â‰¤ total quote volume"

    # Calculate approximate relationships
    df["avg_price"] = df["quote_volume"] / df["volume"].replace(0, np.nan)
    df["avg_price"] = df["avg_price"].fillna(df["close"])

    # Allow for some floating-point error in the approximation
    margin = 0.01  # 1% margin of error
    price_range = df[["open", "high", "low", "close"]].mean(axis=1)

    # Verify that avg_price is within reasonable range of OHLC prices
    assert (
        (df["avg_price"] >= df["low"] * (1 - margin))
        & (df["avg_price"] <= df["high"] * (1 + margin))
    ).all(), "Average price should be within range of OHLC prices"
