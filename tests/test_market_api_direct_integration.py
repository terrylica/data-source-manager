#!/usr/bin/env python
"""Comprehensive market data testing suite.

This module consolidates all market data related tests, including:
1. Data validation (structure, integrity, time windows)
2. API limits and chunking behavior
3. Client integration tests
"""

import pytest
import pytest_asyncio
import aiohttp
import asyncio
import pandas as pd
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, List, Tuple

from core.market_data_client import EnhancedRetriever
from utils.market_constraints import (
    Interval,
    MarketType,
    get_endpoint_url,
    get_market_capabilities,
)
from utils.logger_setup import get_logger

# Configure logging
logger = get_logger(__name__, "INFO", show_path=False, rich_tracebacks=True)

# Test configuration
TEST_SYMBOL = "BTCUSDT"
TEST_INTERVAL = Interval.SECOND_1
BASE_URL = "https://api.binance.com/api/v3/klines"
API_LIMIT = 1000  # Maximum records per request

# Common time windows
FIVE_MINUTES = timedelta(minutes=5)
ONE_HOUR = timedelta(hours=1)
ONE_DAY = timedelta(days=1)


# Fixtures
@pytest_asyncio.fixture
async def api_session():
    """Create an aiohttp ClientSession for API requests."""
    async with aiohttp.ClientSession() as session:
        yield session


@pytest_asyncio.fixture
async def retriever():
    """Fixture providing an EnhancedRetriever instance."""
    async with EnhancedRetriever(market_type=MarketType.SPOT) as client:
        yield client


@pytest.fixture
def reference_time():
    """Fixture providing a consistent reference time for tests."""
    return datetime.now(timezone.utc).replace(microsecond=0)


# Shared validation functions
def validate_market_data_structure(df: pd.DataFrame) -> None:
    """Validate market data structure and types.

    Args:
        df: DataFrame to validate

    Raises:
        AssertionError: If data structure is invalid
    """
    # Column presence and types - support both raw API and market data client formats
    required_columns_raw = [
        "open_time",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "close_time",
        "quote_volume",
        "trades",
        "taker_buy_base_volume",
        "taker_buy_quote_volume",
    ]

    required_columns_client = [
        "open_time",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "close_time",
        "quote_volume",
        "trades",
        "taker_buy_base",
        "taker_buy_quote",
    ]

    # Check if DataFrame matches either format
    has_raw_format = all(col in df.columns for col in required_columns_raw)
    has_client_format = all(col in df.columns for col in required_columns_client)

    assert has_raw_format or has_client_format, (
        f"DataFrame columns don't match either format.\n"
        f"Found: {df.columns.tolist()}\n"
        f"Expected raw API format: {required_columns_raw}\n"
        f"Expected client format: {required_columns_client}"
    )

    # Numeric columns validation
    numeric_cols = ["open", "high", "low", "close", "volume", "quote_volume"]
    for col in numeric_cols:
        assert df[col].dtype == "float64", f"{col} has incorrect data type: {df[col].dtype}"  # type: ignore

    # Integer columns validation - accept int32 or int64
    assert df["trades"].dtype in ["int32", "int64", "float64"], f"trades column has incorrect type: {df['trades'].dtype}"  # type: ignore


def validate_time_integrity(
    df: pd.DataFrame,
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
) -> None:
    """Validate chronological integrity and time windows.

    Args:
        df: DataFrame to validate
        start_time: Optional start time to check
        end_time: Optional end time to check

    Raises:
        AssertionError: If time integrity is invalid
    """
    # Chronological order
    assert df["open_time"].is_monotonic_increasing, "Data is not in chronological order"  # type: ignore

    # Time gaps (for 1s data)
    time_gaps = df["open_time"].diff()[1:] != pd.Timedelta(seconds=1)  # type: ignore
    if time_gaps.any():  # type: ignore
        gap_indices = df.index[time_gaps]  # type: ignore
        gap_times = df["open_time"][gap_indices]  # type: ignore
        logger.warning(f"Found time gaps at: {gap_times.tolist()}")  # type: ignore
        assert False, "Found time gaps in data"  # type: ignore

    # Time boundaries if provided
    if start_time:
        assert df["open_time"].min() >= start_time, "Data starts before requested window"  # type: ignore
    if end_time:
        assert df["open_time"].max() <= end_time, "Data ends after requested window"  # type: ignore


async def fetch_klines(
    session: aiohttp.ClientSession,
    params: Dict[str, Any],
    expected_records: int,
) -> Tuple[List[List[Any]], int]:
    """Fetch kline data with validation.

    Args:
        session: aiohttp session
        params: Request parameters
        expected_records: Expected number of records

    Returns:
        Tuple of (data, actual_records)
    """
    async with session.get(BASE_URL, params=params) as response:
        assert response.status == 200, "API request failed"
        data = await response.json()
        actual_records = len(data)

        # Log request details
        logger.info(
            f"Requested {expected_records} records, received {actual_records} records"
        )

        if actual_records > 0:
            # Log time range
            first_timestamp = datetime.fromtimestamp(data[0][0] / 1000, tz=timezone.utc)
            last_timestamp = datetime.fromtimestamp(data[-1][0] / 1000, tz=timezone.utc)
            logger.info(
                f"Time range: {first_timestamp} to {last_timestamp} ({last_timestamp - first_timestamp})"
            )

        return data, actual_records


# Tests
@pytest.mark.real
@pytest.mark.asyncio
async def test_market_data_integrity(
    api_session: aiohttp.ClientSession, reference_time: datetime
):
    """Test market data integrity with real API data."""
    start_time = reference_time - FIVE_MINUTES

    params = {
        "symbol": TEST_SYMBOL,
        "interval": TEST_INTERVAL.value,
        "startTime": int(start_time.timestamp() * 1000),
        "endTime": int(reference_time.timestamp() * 1000),
        "limit": 300,  # 5 minutes of 1s data
    }

    data, _ = await fetch_klines(api_session, params, 300)

    # Convert to DataFrame with correct column names
    columns = [
        "open_time",
        "open",
        "high",
        "low",
        "close",
        "volume",
        "close_time",
        "quote_volume",
        "trades",
        "taker_buy_base_volume",
        "taker_buy_quote_volume",
        "ignore",
    ]
    df = pd.DataFrame(data, columns=columns)

    # Convert timestamps
    df["open_time"] = pd.to_datetime(df["open_time"], unit="ms", utc=True)  # type: ignore
    df["close_time"] = pd.to_datetime(df["close_time"], unit="ms", utc=True)  # type: ignore

    # Convert numeric columns
    numeric_cols = ["open", "high", "low", "close", "volume", "quote_volume"]
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors="raise")  # type: ignore[assignment]

    df["trades"] = pd.to_numeric(df["trades"], errors="raise").astype("int64")  # type: ignore[assignment]

    # Run validations
    validate_market_data_structure(df)
    validate_time_integrity(df, start_time, reference_time)


@pytest.mark.real
@pytest.mark.asyncio
async def test_api_limits_and_chunking(
    api_session: aiohttp.ClientSession, reference_time: datetime
):
    """Test API limits and chunking behavior with direct API calls."""
    # Test different chunk sizes around the 1000-record limit
    chunk_sizes = [500, 999, 1000, 1001, 1500, 2000]

    for chunk_size in chunk_sizes:
        logger.info(f"\nTesting chunk size: {chunk_size}")
        start_time = reference_time - timedelta(seconds=chunk_size)

        params = {
            "symbol": TEST_SYMBOL,
            "interval": TEST_INTERVAL.value,
            "startTime": int(start_time.timestamp() * 1000),
            "endTime": int(reference_time.timestamp() * 1000),
            "limit": chunk_size,
        }

        data, actual_records = await fetch_klines(api_session, params, chunk_size)

        # Verify record limit enforcement
        if chunk_size <= API_LIMIT:
            assert (
                actual_records == chunk_size
            ), f"For chunk_size={chunk_size}, expected {chunk_size} records but got {actual_records}"
        else:
            assert (
                actual_records == API_LIMIT
            ), f"For chunk_size={chunk_size}, expected {API_LIMIT} records (API limit) but got {actual_records}"

        # Verify data continuity
        if actual_records > 1:
            timestamps = [int(x[0]) for x in data]
            diffs = [
                timestamps[i] - timestamps[i - 1] for i in range(1, len(timestamps))
            ]
            assert all(
                diff == 1000 for diff in diffs
            ), f"Found non-standard time gaps in chunk size {chunk_size}"

        # Rate limit compliance
        await asyncio.sleep(1)


@pytest.mark.real
@pytest.mark.asyncio
async def test_market_data_retrieval(
    retriever: EnhancedRetriever, reference_time: datetime
):
    """Test market data retrieval with validation using EnhancedRetriever."""
    start_time = reference_time - FIVE_MINUTES
    logger.info(f"Testing market data retrieval for {TEST_SYMBOL}")

    df, metadata = await retriever.fetch(
        symbol=TEST_SYMBOL,
        interval=TEST_INTERVAL,
        start_time=start_time,
        end_time=reference_time,
    )

    # Convert column names if needed
    if "taker_buy_volume" in df.columns:
        df = df.rename(
            columns={
                "taker_buy_volume": "taker_buy_base_volume",
                "timestamp": "open_time",
            }
        )

    # Validate data structure and integrity
    validate_market_data_structure(df)
    validate_time_integrity(df, start_time, reference_time)

    # Validate metadata
    assert metadata["total_records"] > 0, "No records retrieved"
    assert metadata["chunks_failed"] == 0, "Some chunks failed to download"


@pytest.mark.real
@pytest.mark.asyncio
async def test_large_data_retrieval(
    retriever: EnhancedRetriever, reference_time: datetime
):
    """Test retrieval of larger datasets."""
    start_time = reference_time - ONE_HOUR
    logger.info(f"Testing large data retrieval for {TEST_SYMBOL}")

    df, metadata = await retriever.fetch(
        symbol=TEST_SYMBOL,
        interval=TEST_INTERVAL,
        start_time=start_time,
        end_time=reference_time,
    )

    # Convert column names if needed
    if "taker_buy_volume" in df.columns:
        df = df.rename(
            columns={
                "taker_buy_volume": "taker_buy_base_volume",
                "timestamp": "open_time",
            }
        )

    # Validate chunking behavior
    assert (
        metadata["chunks_processed"] > 1
    ), "Large dataset should be processed in chunks"
    assert metadata["chunks_failed"] == 0, "No chunks should fail"

    # Validate data
    validate_market_data_structure(df)
    validate_time_integrity(df, start_time, reference_time)
