#!/usr/bin/env python
"""Validation utilities for DataSourceManager tests.

This module contains common validation functions used across multiple test files
to ensure consistent validation behavior and reduce code duplication.
"""

import pandas as pd
from datetime import timezone
from typing import Any, Optional

from utils.logger_setup import get_logger

logger = get_logger(__name__, "INFO", show_path=False, rich_tracebacks=True)


def validate_dataframe_structure(
    df: pd.DataFrame, allow_empty: bool = True, name: str = "DataFrame"
) -> None:
    """Validate DataFrame structure with detailed logging.

    Args:
        df: DataFrame to validate
        allow_empty: Whether empty DataFrames are acceptable
        name: Name of the DataFrame for logging purposes

    Raises:
        AssertionError: If validation fails
    """
    logger.info(
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
    logger.info(f"â•‘ Structure Validation: {name}")
    logger.info(
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )

    # Empty Check
    if df.empty and not allow_empty:
        logger.error("â•‘ âŒ DataFrame is empty when it should contain data")
        raise AssertionError(f"{name} should not be empty")
    elif df.empty:
        logger.info("â•‘ â„¹ï¸  DataFrame is empty (allowed)")
        logger.info(
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )
        return

    # Index Validation
    logger.info("â•‘ Index Validation:")
    if isinstance(df.index, pd.DatetimeIndex):
        logger.info("â•‘ âœ“ Index is DatetimeIndex")
    else:
        logger.error(f"â•‘ âŒ Index is {type(df.index).__name__}, expected DatetimeIndex")
        raise AssertionError(f"{name} index should be DatetimeIndex")

    if df.index.tz == timezone.utc:
        logger.info("â•‘ âœ“ Timezone is UTC")
    else:
        logger.error(f"â•‘ âŒ Timezone is {df.index.tz}, expected UTC")
        raise AssertionError(f"{name} index should be UTC")

    if df.index.is_monotonic_increasing:
        logger.info("â•‘ âœ“ Index is monotonically increasing")
    else:
        logger.error("â•‘ âŒ Index is not monotonically increasing")
        raise AssertionError(f"{name} index should be monotonically increasing")

    # Column Validation
    logger.info("â•‘")
    logger.info("â•‘ Column Validation:")
    required_columns = {
        "open",
        "high",
        "low",
        "close",
        "volume",
        "close_time",
        "quote_volume",
        "trades",
        "taker_buy_volume",
        "taker_buy_quote_volume",
    }
    missing_columns = required_columns - set(df.columns)
    if missing_columns:
        logger.error(f"â•‘ âŒ Missing required columns: {missing_columns}")
        raise AssertionError(f"Missing required columns: {missing_columns}")
    logger.info("â•‘ âœ“ All required columns present")

    logger.info(
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )


def log_dataframe_info(
    df: pd.DataFrame, source: str, to_arrow_fn: Optional[Any] = None
) -> None:
    """Log detailed DataFrame information for analysis.

    Args:
        df: DataFrame to analyze
        source: Source description for the DataFrame
        to_arrow_fn: Optional function to convert timestamps to Arrow objects
    """
    logger.info(
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
    logger.info(f"â•‘ ğŸ“Š DATA ANALYSIS REPORT - {source}")
    logger.info(
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )

    if df.empty:
        logger.warning("â•‘ âš ï¸  DataFrame is empty!")
        logger.info(
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )
        return

    # Basic Information
    logger.info("â•‘ ğŸ“Œ Basic Information:")
    logger.info(f"â•‘   â€¢ ğŸ“‘ Records: {df.shape[0]:,}")
    logger.info(f"â•‘   â€¢ ğŸ“Š Columns: {df.shape[1]}")
    logger.info(f"â•‘   â€¢ ğŸ”‘ Index Type: {type(df.index).__name__}")
    if isinstance(df.index, pd.DatetimeIndex):
        logger.info(f"â•‘   â€¢ ğŸŒ Timezone: {df.index.tz or 'naive'}")
    else:
        logger.info("â•‘   â€¢ ğŸŒ Timezone: N/A (not a DatetimeIndex)")

    # Time Range Analysis
    logger.info("â•‘")
    logger.info("â•‘ â° Time Range Analysis:")

    # Handle timestamp conversion based on whether to_arrow_fn is provided
    if to_arrow_fn and not df.empty:
        first_ts = to_arrow_fn(df.index[0])
        last_ts = to_arrow_fn(df.index[-1])
        logger.info(
            f"â•‘   â€¢ ğŸ”µ First Record: {first_ts.format('YYYY-MM-DD HH:mm:ss')} UTC"
        )
        logger.info(
            f"â•‘   â€¢ ğŸ”´ Last Record: {last_ts.format('YYYY-MM-DD HH:mm:ss')} UTC"
        )
        logger.info(f"â•‘   â€¢ âŒ› Total Duration: {last_ts - first_ts}")
    elif not df.empty:
        logger.info(f"â•‘   â€¢ ğŸ”µ First Record: {df.index[0]}")
        logger.info(f"â•‘   â€¢ ğŸ”´ Last Record: {df.index[-1]}")
        logger.info(f"â•‘   â€¢ âŒ› Total Duration: {df.index[-1] - df.index[0]}")

    # Data Quality Metrics
    logger.info("â•‘")
    logger.info("â•‘ ğŸ” Data Quality Metrics:")
    logger.info(f"â•‘   â€¢ âŒ Missing Values: {df.isnull().sum().sum():,}")
    logger.info(f"â•‘   â€¢ ğŸ”„ Duplicate Timestamps: {df.index.duplicated().sum():,}")

    # Price Statistics
    logger.info("â•‘")
    logger.info("â•‘ ğŸ’¹ Price Statistics:")
    logger.info(
        f"â•‘   â€¢ ğŸ’° Price Range: ${df['low'].min():,.2f} â†’ ${df['high'].max():,.2f}"
    )
    logger.info(f"â•‘   â€¢ ğŸ“ˆ Average Volume: {df['volume'].mean():,.2f}")
    logger.info(f"â•‘   â€¢ ğŸ”„ Total Trades: {df['trades'].sum():,}")

    # Data Types
    logger.info("â•‘")
    logger.info("â•‘ ğŸ”§ Column Data Types:")
    for col, dtype in df.dtypes.items():
        logger.info(f"â•‘   â€¢ {col}: {dtype}")

    logger.info(
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    )
