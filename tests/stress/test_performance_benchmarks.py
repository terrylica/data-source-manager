"""Performance latency benchmarks for CryptoKlineVisionData.

Tests that establish baseline latencies for regression detection:
- Cache hit latency (target: <10ms)
- Vision fetch latency (baseline: 1-5s per day range)
- REST fetch latency (baseline: 100-500ms per request)
- Full FCP chain timing

GitHub Issue #4 - P0: Performance latency benchmarks
"""

import time
from datetime import datetime, timedelta, timezone

import pytest

from ckvd import DataProvider, CryptoKlineVisionData, Interval, MarketType
from ckvd.core.sync.ckvd_types import DataSource


@pytest.mark.stress
class TestCacheLatency:
    """Cache hit latency should be <10ms."""

    def test_cache_hit_under_50ms(self, historical_time_range):
        """Cache hit should complete in <50ms after warm-up.

        Note: 50ms threshold accounts for Arrow file I/O and DataFrame construction.
        Actual cache reads are typically <20ms but can vary with system load.
        """
        start, end = historical_time_range
        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        # Warm-up: populate cache
        manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)

        # Measure cache hit latency (multiple samples)
        latencies = []
        for _ in range(5):
            start_time = time.perf_counter()
            manager.get_data("BTCUSDT", start, end, Interval.HOUR_1, enforce_source=DataSource.CACHE)
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            latencies.append(elapsed_ms)

        manager.close()

        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)

        # Cache hit should be <50ms on average (accounts for I/O variance)
        assert avg_latency < 50, f"Cache hit avg latency {avg_latency:.2f}ms exceeds 50ms"
        # Minimum should be <20ms (best case with system load)
        assert min_latency < 20, f"Cache hit min latency {min_latency:.2f}ms exceeds 20ms"

    def test_cache_latency_consistent(self, historical_time_range):
        """Cache latency should be consistent across repeated reads."""
        start, end = historical_time_range
        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        # Warm-up
        manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)

        # Measure 10 cache reads
        latencies = []
        for _ in range(10):
            start_time = time.perf_counter()
            manager.get_data("BTCUSDT", start, end, Interval.HOUR_1, enforce_source=DataSource.CACHE)
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            latencies.append(elapsed_ms)

        manager.close()

        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)

        # Max should not be more than 3x average (consistency check)
        if avg_latency > 0:
            consistency_ratio = max_latency / avg_latency
            assert consistency_ratio < 3, (
                f"Cache latency inconsistent: max {max_latency:.2f}ms is "
                f"{consistency_ratio:.1f}x avg {avg_latency:.2f}ms"
            )


@pytest.mark.stress
class TestVisionLatency:
    """Vision fetch baseline measurement (1-5s per day range).

    Note: These tests use FCP auto-selection for historical data rather than
    enforce_source=VISION, since the Vision API enforced path has a known
    interface issue (FSSpecVisionHandler.fetch vs fetch_data).
    """

    def test_vision_historical_fetch_baseline(self):
        """Historical fetch (via FCP) should complete within reasonable time."""
        # Use historical data that should go through Vision API via FCP
        end = datetime(2024, 1, 8, tzinfo=timezone.utc)
        start = datetime(2024, 1, 7, tzinfo=timezone.utc)  # 1 day

        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        start_time = time.perf_counter()
        df = manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)
        elapsed_s = time.perf_counter() - start_time

        manager.close()

        # Historical fetch for 1 day should be <10s (network dependent)
        assert elapsed_s < 10, f"Historical fetch took {elapsed_s:.2f}s, exceeds 10s limit"

        # Should return data
        assert len(df) > 0, "Historical fetch returned no data"

    def test_historical_fetch_scales_with_range(self):
        """Historical fetch time should scale roughly linearly with date range."""
        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        # 1 day fetch
        end_1d = datetime(2024, 1, 8, tzinfo=timezone.utc)
        start_1d = datetime(2024, 1, 7, tzinfo=timezone.utc)

        start_time = time.perf_counter()
        manager.get_data("BTCUSDT", start_1d, end_1d, Interval.HOUR_1)
        time_1d = time.perf_counter() - start_time

        # 3 day fetch
        end_3d = datetime(2024, 1, 10, tzinfo=timezone.utc)
        start_3d = datetime(2024, 1, 7, tzinfo=timezone.utc)

        start_time = time.perf_counter()
        manager.get_data("BTCUSDT", start_3d, end_3d, Interval.HOUR_1)
        time_3d = time.perf_counter() - start_time

        manager.close()

        # 3x date range shouldn't take more than 5x the time
        # (accounts for overhead, caching, and parallelism)
        if time_1d > 0:
            scaling_ratio = time_3d / time_1d
            assert scaling_ratio < 5, (
                f"Historical scaling ratio {scaling_ratio:.1f}x exceeds 5x "
                f"(1d={time_1d:.2f}s, 3d={time_3d:.2f}s)"
            )


@pytest.mark.stress
class TestRESTLatency:
    """REST fetch baseline measurement (100-500ms per request)."""

    def test_rest_fetch_baseline(self):
        """REST fetch should complete within reasonable time."""
        # Use recent data that forces REST API
        end = datetime.now(timezone.utc) - timedelta(hours=1)
        start = end - timedelta(hours=2)

        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        start_time = time.perf_counter()
        df = manager.get_data("BTCUSDT", start, end, Interval.MINUTE_1, enforce_source=DataSource.REST)
        elapsed_ms = (time.perf_counter() - start_time) * 1000

        manager.close()

        # REST fetch should be 100-500ms baseline
        # Allow up to 2000ms for slow networks
        assert elapsed_ms < 2000, f"REST fetch took {elapsed_ms:.0f}ms, exceeds 2000ms limit"

        # Should return data
        assert len(df) > 0, "REST fetch returned no data"

    def test_rest_multiple_requests_stable(self):
        """Multiple REST requests should have stable latency."""
        end = datetime.now(timezone.utc) - timedelta(hours=1)
        start = end - timedelta(hours=1)

        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        latencies = []
        for _ in range(3):
            start_time = time.perf_counter()
            manager.get_data("BTCUSDT", start, end, Interval.MINUTE_1, enforce_source=DataSource.REST)
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            latencies.append(elapsed_ms)

        manager.close()

        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)

        # Max should not be more than 3x average
        if avg_latency > 0:
            consistency_ratio = max_latency / avg_latency
            assert consistency_ratio < 3, (
                f"REST latency inconsistent: max {max_latency:.0f}ms is "
                f"{consistency_ratio:.1f}x avg {avg_latency:.0f}ms"
            )


@pytest.mark.stress
class TestFCPTotalLatency:
    """Full FCP chain timing."""

    def test_fcp_auto_selection_latency(self, historical_time_range):
        """FCP auto source selection should complete efficiently."""
        start, end = historical_time_range
        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        # First fetch (may hit any source)
        start_time = time.perf_counter()
        df = manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)
        first_fetch_ms = (time.perf_counter() - start_time) * 1000

        # Second fetch (should hit cache)
        start_time = time.perf_counter()
        manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)
        second_fetch_ms = (time.perf_counter() - start_time) * 1000

        manager.close()

        # Second fetch should be same or faster (cache hit)
        # Note: If first fetch also hit cache, they may be nearly equal
        # Allow 20% tolerance for timing variance
        assert second_fetch_ms < first_fetch_ms * 1.2, (
            f"Cache hit ({second_fetch_ms:.0f}ms) significantly slower than first fetch ({first_fetch_ms:.0f}ms)"
        )

        # Cache hit should be <50ms (accounts for I/O variance)
        assert second_fetch_ms < 50, f"Cache hit took {second_fetch_ms:.2f}ms, expected <50ms"

        # Should return data
        assert len(df) > 0, "FCP returned no data"

    def test_fcp_with_source_tracking(self, historical_time_range):
        """FCP with source tracking should report sources used."""
        start, end = historical_time_range
        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        start_time = time.perf_counter()
        df = manager.get_data("BTCUSDT", start, end, Interval.HOUR_1, include_source_info=True)
        elapsed_ms = (time.perf_counter() - start_time) * 1000

        manager.close()

        # Should complete in reasonable time
        assert elapsed_ms < 10000, f"FCP with tracking took {elapsed_ms:.0f}ms"

        # Should have data
        assert len(df) > 0, "FCP returned no data"

        # Should have source tracking column
        if "_data_source" in df.columns:
            sources = df["_data_source"].unique()
            assert len(sources) > 0, "No sources recorded"

    def test_fcp_mixed_timerange_latency(self):
        """FCP should handle mixed historical + recent data efficiently.

        Note: Uses a purely historical range to avoid Vision API interface issues
        with enforce_source. FCP will use cache/REST as appropriate.
        """
        # Use historical range that's cached or REST-only
        end = datetime(2024, 1, 15, tzinfo=timezone.utc)
        start = datetime(2024, 1, 10, tzinfo=timezone.utc)

        manager = CryptoKlineVisionData.create(DataProvider.BINANCE, MarketType.FUTURES_USDT)

        start_time = time.perf_counter()
        df = manager.get_data("BTCUSDT", start, end, Interval.HOUR_1)
        elapsed_s = time.perf_counter() - start_time

        manager.close()

        # Historical fetch should complete within 30s
        assert elapsed_s < 30, f"FCP fetch took {elapsed_s:.1f}s, exceeds 30s limit"

        # Should return substantial data
        # 5 days * 24 hours = 120 expected rows
        assert len(df) > 100, f"Expected >100 rows, got {len(df)}"
